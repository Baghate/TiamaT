{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aac7f8d-f982-4ac8-835c-fd4cbb383f78",
   "metadata": {},
   "source": [
    "# Correction from Label Studio and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601803b8-ff63-49ce-8ed0-94eea362e577",
   "metadata": {},
   "source": [
    "This notebook SHOULD NOT be launched until any corrections have been made in Label Studio.\n",
    "The purpose of the various functions is to be able to extract the data to be analysed after training in order to assess the robustness of the model.\n",
    "\n",
    "**Warning 1**\n",
    "The requirement is to use, or create, a Label Studio account: (https://labelstud.io/guide/install.html) and to have followed the workflow of the previous notebooks or at least to have created similar files.\n",
    "\n",
    "**Warning 2**\n",
    "The export format for corrections from Label Studio MUST be csv ONLY.\n",
    "Label Studio's YOLO export format does not allow you to keep the names of the images as they were imported:  in the context of this workflow, these are the URLs of the images from which this script can retrieve the name of the image and generate .txt files with the same name as the image (thus allowing you to use this new data for a new training session).\n",
    "\n",
    "**Warning 3**\n",
    "It is essential to complete the \"Labeling Interface\" by specifying strictly the same class names (case, special characters, etc.) as those declared in the .json file.\n",
    "\n",
    "**Warning 4**\n",
    "If there is some issue in Label Studio, you can change the labeling results' name from \"predictions\" to \"annotations\", since Label documentation explain that you can't change the bounding box coordinates for prediction, only for detection : https://labelstud.io/guide/predictions.html#Predictions-are-read-only.\n",
    "But I tried and you can change the predicted bounding boxes and export the new data with the changes without any problem.\n",
    "\n",
    "**Notice concerning use** \n",
    "Any use, even partial, of the content of this notebook must be accompanied by an appropriate citation.\n",
    "\n",
    "&copy; 2023 Marion Charpier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ab46e-2d24-49d9-853a-5f6d136ecb5d",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f71f0ae-921e-42db-a318-1061a5c2afe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join('..', 'modules'))\n",
    "\n",
    "from class_names_functions import get_labels, get_class_name, get_class_code\n",
    "from transform_coordinates_functions import from_ls_to_yolo\n",
    "from folders_path import get_results_folder\n",
    "from manipulate_files import open_json_file, save_json_file, get_files, exclude_training_images, load_data_from_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488719b-87fa-4946-99a7-0bee1799580e",
   "metadata": {},
   "source": [
    "## Get the Label Studio correction in new .txt files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac47a6-af7d-4dea-ab15-45cebe69157d",
   "metadata": {},
   "source": [
    "### Generate new txt files with correct bounding boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "73613c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1890', '1891', '1892', '1886', '1888', '1889', '1887']\n",
      "Corrected annotations have been successfully converted and saved to to the results folder /Users/marioncharpier/Documents/TORNE-H/GitHub/TiamaT/output/runs/predict/dragon_project_20250418_1n_i640_e10_b-1_w8/correctedLabels.\n"
     ]
    }
   ],
   "source": [
    "def get_corrected_label(img_dataset_folder, yolo_model_folder):\n",
    "    \n",
    "    corrections_folder = img_dataset_folder.replace('image_inputs/eval_images', 'annotations/prediction_corrections')\n",
    "\n",
    "    # Recompose the path to the model results folder\n",
    "    results_folder = os.path.join(\n",
    "            os.path.dirname(os.path.dirname(yolo_model_folder)), \n",
    "            'predict',\n",
    "            img_dataset_folder.split('/')[-3] + '_' + os.path.basename(yolo_model_folder))\n",
    "\n",
    "    # If it doesn't exist, create the \"correctedLabels\" folder for corrected labels\n",
    "    os.makedirs(os.path.join(results_folder, 'correctedLabels'), exist_ok = True)\n",
    "\n",
    "    # Retrieve labels from the model folder\n",
    "    labels = get_labels(os.path.join(yolo_model_folder, 'labels.txt'))\n",
    "\n",
    "    # Retrieve corrected JSON files as a list and open them\n",
    "    corrected_files = [file for file in os.listdir(corrections_folder) if not file.startswith('.')]\n",
    "    print(corrected_files)\n",
    "    \n",
    "    for corrected_file in corrected_files:\n",
    "        corrections = open_json_file(os.path.join(corrections_folder, corrected_file))\n",
    "\n",
    "        for result_item in corrections['result']:\n",
    "            result_item.pop('score', None)\n",
    "        save_json_file(os.path.join(corrections_folder, corrected_file), corrections)\n",
    "        \n",
    "        # Retrieve image name from corrected annotations file\n",
    "        name = corrections['task']['data']['image']\n",
    "        img_name = '.'.join(os.path.basename(name).split('.')[:-1])\n",
    "        result = corrections['result']\n",
    "\n",
    "        # Check if an annotation box has been deleted during correction\n",
    "        for item in result:\n",
    "            if \"id\" not in item:\n",
    "                print(\"Prediction box erased.\")\n",
    "        \n",
    "            else:\n",
    "                # Create a .txt file with annotation data\n",
    "                with open(os.path.join(results_folder, 'correctedLabels', img_name + '.txt'), 'w') as yolo_correction:\n",
    "                    for i, result_item in enumerate(result):\n",
    "                        \n",
    "                        # Retrieve annotation box coordinates\n",
    "                        value = result_item['value']\n",
    "                        x, y, w, h = from_ls_to_yolo(value['x'], value['y'], value['width'], value['height'])\n",
    "\n",
    "                        # Retrieve the annotation label and associate it with its number in the \"labels.txt\" file\n",
    "                        class_name = value['rectanglelabels'][0]\n",
    "                        class_id = get_class_code(class_name, labels)\n",
    "                        \n",
    "                        yolo_correction.write(f\"{class_id} {x} {y} {w} {h}\\n\")\n",
    "                        \n",
    "        # print(f\"Corrections made to the {img_name} image file have been saved.\")\n",
    "                        \n",
    "    print(f\"Corrected annotations have been successfully converted and saved to to the results folder {os.path.join(results_folder, 'correctedLabels')}.\")\n",
    "\n",
    "\n",
    "# Generate the corrected files in YOLO format\n",
    "get_corrected_label(img_dataset_folder, yolo_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6e28d723-88bd-4a37-8b6c-1ad8528ca017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_corrected_label_files(img_dataset_folder, yolo_model_folder):\n",
    "    \"\"\"\n",
    "    This function processes corrected annotation files in JSON format and converts them into YOLO-compatible `.txt` files.\n",
    "    It ensures that any corrections made to bounding boxes and labels are reflected in the YOLO format, \n",
    "    and saves the corrected annotations in a `correctedLabels` folder.\n",
    "\n",
    "    :param img_dataset_folder: \n",
    "        - Type: str\n",
    "        - Description: The path to the folder containing the dataset images (unannotated images). \n",
    "                       This folder is used to locate the corresponding corrections folder.\n",
    "    :param yolo_model_folder: \n",
    "        - Type: str\n",
    "        - Description: The path to the folder containing the trained YOLO model. This folder is used to \n",
    "                       access the `labels.txt` file for retrieving class names and IDs.\n",
    "    \n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It creates `.txt` files with the corrected \n",
    "                       annotations in YOLO format and saves them in the `correctedLabels` subdirectory of \n",
    "                       the results folder.\n",
    "\n",
    "    This function automates the conversion of corrected annotations into a format suitable for YOLO training, \n",
    "    ensuring that all changes made during annotation review are correctly applied.\n",
    "    \"\"\"\n",
    "\n",
    "    corrections_folder = img_dataset_folder.replace('image_inputs/eval_images', 'annotations/prediction_corrections')\n",
    "\n",
    "    # Recompose the path to the model results folder\n",
    "    results_folder = os.path.join(\n",
    "            os.path.dirname(os.path.dirname(yolo_model_folder)), \n",
    "            'predict',\n",
    "            img_dataset_folder.split('/')[-3] + '_' + os.path.basename(yolo_model_folder))\n",
    "\n",
    "    # If it doesn't exist, create the \"correctedLabels\" folder for corrected labels\n",
    "    os.makedirs(os.path.join(results_folder, 'correctedLabels'), exist_ok = True)\n",
    "\n",
    "    # Retrieve labels from the model folder\n",
    "    labels = get_labels(os.path.join(yolo_model_folder, 'labels.txt'))\n",
    "\n",
    "    # Retrieve corrected JSON files as a list and open them\n",
    "    corrected_files = [file for file in os.listdir(corrections_folder) if not file.startswith('.')]\n",
    "    \n",
    "    for corrected_file in corrected_files:\n",
    "        corrections = open_json_file(os.path.join(corrections_folder, corrected_file))\n",
    "\n",
    "        for result_item in corrections['result']:\n",
    "            result_item.pop('score', None)\n",
    "        save_json_file(os.path.join(corrections_folder, corrected_file), corrections)\n",
    "        \n",
    "        # Retrieve image name from corrected annotations file\n",
    "        name = corrections['task']['data']['image']\n",
    "        img_name = '.'.join(os.path.basename(name).split('.')[:-1])\n",
    "        result = corrections['result']\n",
    "\n",
    "        # Check if an annotation box has been deleted during correction\n",
    "        for item in result:\n",
    "            if \"id\" not in item:\n",
    "                print(\"Prediction box erased.\")\n",
    "        \n",
    "            else:\n",
    "                # Create a .txt file with annotation data\n",
    "                with open(os.path.join(results_folder, 'correctedLabels', img_name + '.txt'), 'w') as yolo_correction:\n",
    "                    for i, result_item in enumerate(result):\n",
    "                        \n",
    "                        # Retrieve annotation box coordinates\n",
    "                        value = result_item['value']\n",
    "                        x, y, w, h = from_ls_to_yolo(value['x'], value['y'], value['width'], value['height'])\n",
    "\n",
    "                        # Retrieve the annotation label and associate it with its number in the \"labels.txt\" file\n",
    "                        class_name = value['rectanglelabels'][0]\n",
    "                        class_id = get_class_code(class_name, labels)\n",
    "                        \n",
    "                        yolo_correction.write(f\"{class_id} {x} {y} {w} {h}\\n\")\n",
    "                        \n",
    "        # print(f\"Corrections made to the {img_name} image file have been saved.\")\n",
    "                        \n",
    "    print(f\"Corrected annotations have been successfully converted and saved to to the results folder {os.path.join(results_folder, 'correctedLabels')}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41ad14-38fd-4fab-96f8-84b74dca97b1",
   "metadata": {},
   "source": [
    "## Get results in csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b345c-c591-4a41-a0ea-20f2841e93e5",
   "metadata": {},
   "source": [
    "### Get a list of images used for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e88d4d76-793c-4428-92d6-fbf901bb7de5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_from_training(yolo_model_folder, img_dataset_folder):\n",
    "    \"\"\"\n",
    "    This function returns a list of images that may have been used to train a YOLO model. \n",
    "    It compares the images in the dataset folder with those listed in the `traindata.txt` file \n",
    "    found in the `dataset_statistics` subdirectory of the YOLO model folder.\n",
    "\n",
    "    **Warning**: This function assumes that the training and prediction data have similar naming conventions \n",
    "    and that the `traindata.txt` file accurately reflects the images used during training.\n",
    "\n",
    "    :param yolo_model_folder: \n",
    "        - Type: str\n",
    "        - Description: The absolute path to the folder containing the YOLO model and its associated training data. \n",
    "                       The function uses the `traindata.txt` file in the `dataset_statistics` subdirectory of this folder \n",
    "                       to retrieve the list of training images.\n",
    "    :param img_dataset_folder: \n",
    "        - Type: str\n",
    "        - Description: The path to the folder containing the dataset images. This folder is searched to find images \n",
    "                       that match those listed in the `traindata.txt` file.\n",
    "    \n",
    "    :return: \n",
    "        - Type: list\n",
    "        - Description: A list of image filenames that were used to train the model. If no matching images are found, \n",
    "                       an empty list is returned.\n",
    "\n",
    "    This function is useful for verifying whether specific images in a dataset were part of the training process, \n",
    "    helping ensure consistency between training and inference datasets.\n",
    "    \"\"\"    \n",
    "    train_data_list = os.path.join(yolo_model_folder, 'dataset_statistics/training_dataset.txt')\n",
    "    \n",
    "    train_data = []\n",
    "    \n",
    "    with open(train_data_list, 'r') as train_data_file:\n",
    "        for line in train_data_file:\n",
    "            img_path = line.strip()\n",
    "            train_data.append(img_path)\n",
    "      \n",
    "    image_files = [file for file in os.listdir(img_dataset_folder) if file.endswith(('.jpg', '.png', 'tiff'))]\n",
    "    \n",
    "    matching_images = [image_name for image_name in train_data if os.path.basename(image_name) in image_files]\n",
    "   \n",
    "    if matching_images:\n",
    "        print(\"The following images were used to train the model:\")\n",
    "        print(matching_images)\n",
    "        \n",
    "    else:\n",
    "        print(f\"No images in the {img_dataset_folder} folder were used to train the model {os.path.basename(yolo_model_folder)}.\")\n",
    "    \n",
    "    return matching_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dce9be-022f-49f1-b043-e2841fba7fc5",
   "metadata": {},
   "source": [
    "### Calculate IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca6562-b757-4b62-abf2-e570848b7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    This function calculates the Intersection over Union (IoU) between two bounding boxes. IoU is a measure \n",
    "    of the overlap between two bounding boxes and is commonly used to evaluate the accuracy of object detection models.\n",
    "\n",
    "    The function is adapted from the 'bb_intersection_over_union' function on PyImageSearch, which uses \n",
    "    bounding box coordinates in (x_min, y_min, x_max, y_max) format. The adaptation accounts for the \n",
    "    fact that YOLOv8 provides bounding box coordinates in relative format (x_center, y_center, width, height).\n",
    "\n",
    "    :param box1: \n",
    "        - Type: list or tuple\n",
    "        - Description: The first bounding box defined as a list or tuple of values [class_id, x_center, y_center, width, height]. \n",
    "                       The coordinates are relative to the image dimensions.\n",
    "    :param box2: \n",
    "        - Type: list or tuple\n",
    "        - Description: The second bounding box defined as a list or tuple of values [class_id, x_center, y_center, width, height]. \n",
    "                       The coordinates are relative to the image dimensions.\n",
    "    \n",
    "    :return: \n",
    "        - Type: float\n",
    "        - Description: The IoU value, which ranges from 0 to 1. A value of 0 indicates no overlap, \n",
    "                       while a value of 1 indicates perfect overlap between the two bounding boxes.\n",
    "\n",
    "    This function is useful for evaluating object detection models and determining how well the predicted bounding boxes \n",
    "    match the ground truth annotations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert coordinates (x, y, w, h) in (x_min, y_min, x_max, y_max)\n",
    "    box1_x_min = box1[1] - box1[3] / 2\n",
    "    box1_y_min = box1[2] - box1[4] / 2\n",
    "    box1_x_max = box1[1] + box1[3] / 2\n",
    "    box1_y_max = box1[2] + box1[4] / 2\n",
    "    \n",
    "    box2_x_min = box2[1] - box2[3] / 2\n",
    "    box2_y_min = box2[2] - box2[4] / 2\n",
    "    box2_x_max = box2[1] + box2[3] / 2\n",
    "    box2_y_max = box2[2] + box2[4] / 2\n",
    "    \n",
    "    # Calculate coordinates (x,y) of the overlap\n",
    "    x_min = max(box1_x_min, box2_x_min)\n",
    "    y_min = max(box1_y_min, box2_y_min)\n",
    "    x_max = min(box1_x_max, box2_x_max)\n",
    "    y_max = min(box1_y_max, box2_y_max)\n",
    "    \n",
    "    # Calculate the area of the overlap\n",
    "    intersection_area = max(0, x_max - x_min + 1) * max(0, y_max - y_min + 1)\n",
    "\n",
    "    # Calculer the area of the two bounding boxes\n",
    "    box1_area = (box1_x_max - box1_x_min + 1) * (box1_y_max - box1_y_min + 1)\n",
    "    box2_area = (box2_x_max - box2_x_min + 1) * (box2_y_max - box2_y_min + 1)\n",
    "    \n",
    "    # Calculate the Intersection over Union (IoU)\n",
    "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a72d7-cbbd-40cd-b7b1-ca391a8aa2ff",
   "metadata": {},
   "source": [
    "### Get the best iou macthes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb0eb7d1-a69d-41ee-b422-a20d1f37e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_iou_matches(predictions, corrected_predictions):\n",
    "    \"\"\"\n",
    "    This function finds the best matching corrected bounding box for each predicted bounding box based on \n",
    "    the Intersection over Union (IoU) value. For each prediction, it calculates the IoU with all corrected \n",
    "    bounding boxes and selects the one with the highest IoU as the best match.\n",
    "    \n",
    "    :param predictions: \n",
    "        - Type: list of str\n",
    "        - Description: A list of predicted bounding boxes in YOLO format (class_id, x_center, y_center, width, height). \n",
    "                       Each bounding box is represented as a string of space-separated values.\n",
    "    :param corrected_predictions: \n",
    "        - Type: list of str\n",
    "        - Description: A list of corrected bounding boxes in YOLO format (class_id, x_center, y_center, width, height). \n",
    "                       Each bounding box is represented as a string of space-separated values.\n",
    "    \n",
    "    :return: \n",
    "        - Type: list of tuples\n",
    "        - Description: A list of tuples, where each tuple contains:\n",
    "            - The predicted bounding box (str)\n",
    "            - The best matching corrected bounding box (str) based on the highest IoU\n",
    "            - The IoU value (float) for the best match\n",
    "    \n",
    "    This function is useful for evaluating the performance of a model by comparing its predictions with manually corrected \n",
    "    ground truth annotations, identifying the best matches based on spatial overlap.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an empty list for the best matches\n",
    "    best_matches = []\n",
    "\n",
    "    for prediction in predictions:\n",
    "        prediction_box = prediction.split(\" \")\n",
    "        prediction_box = [float(coord) for coord in prediction_box]\n",
    "        best_iou = 0\n",
    "        best_correction = None\n",
    "\n",
    "        for correction in corrected_predictions:\n",
    "            correction_box = correction.split(\" \")\n",
    "            correction_box = [float(coord) for coord in correction_box]\n",
    "\n",
    "            iou = calculate_iou(prediction_box, correction_box)\n",
    "            \n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_correction = correction\n",
    "        \n",
    "        best_matches.append((prediction, best_correction, best_iou))\n",
    "    \n",
    "    return best_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f499f0-b1d4-4a7c-afea-ee0ab341cd4a",
   "metadata": {},
   "source": [
    "### Load data from files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb5167-9dbd-473b-93fb-ecfb8c6629e4",
   "metadata": {},
   "source": [
    "### Save results in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f470247f-556b-4125-82ca-6826eb3b3fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(rows, output_file):\n",
    "    \"\"\"\n",
    "    This function saves a list of generated and corrected annotations into a CSV file. If no annotations are provided, \n",
    "    it logs a message indicating that no corrections were made and exits the function. Otherwise, it creates a \n",
    "    DataFrame from the provided data, sorts it by the 'Filename' column, and writes it to the specified CSV file.\n",
    "    \n",
    "    :param rows: \n",
    "        - Type: list of dict\n",
    "        - Description: A list of dictionaries containing the generated and corrected annotations. Each dictionary should \n",
    "                       represent a single annotation entry with keys as column names.\n",
    "    :param output_file: \n",
    "        - Type: str\n",
    "        - Description: The path where the CSV file will be created. This file will store the sorted annotations for easy review.\n",
    "    \n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It either creates the CSV file or prints a message if no \n",
    "                       annotations are provided.\n",
    "    \n",
    "    This function is useful for storing annotation results in a structured format, facilitating further analysis or \n",
    "    review of corrected and generated annotations.\n",
    "    \"\"\"\n",
    "\n",
    "    if not rows:\n",
    "        print('No correction made')\n",
    "        return\n",
    "    df = pd.DataFrame(rows)\n",
    "    df_sorted = df.sort_values('Filename')\n",
    "    df_sorted.to_csv(output_file, sep=';',index=False)\n",
    "    print(f\"The {output_file} file has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2313e7c6-d84a-4e7f-b8d5-897b48fd2e60",
   "metadata": {},
   "source": [
    "### Generate the csv with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc007e-a496-4d5d-bcde-46b3d62c2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_results(img_dataset_folder, yolo_model_folder, all_results):\n",
    "    \"\"\"\n",
    "    This function generates correction results in CSV format by evaluating each annotation as True Positive (TP), \n",
    "    False Positive (FP), or False Negative (FN). It compares the predictions made by a YOLO model against \n",
    "    manually corrected annotations and stores the results in a CSV file for further analysis.\n",
    "    \n",
    "    :param img_dataset_folder: \n",
    "        - Type: str\n",
    "        - Description: The path to the folder containing the dataset images. This folder is used to locate the \n",
    "                       corresponding results and corrections.\n",
    "    :param yolo_model_folder: \n",
    "        - Type: str\n",
    "        - Description: The path to the folder containing the YOLO model and its associated data. This folder \n",
    "                       is used to locate labels and other files related to the model.\n",
    "    :param all_results: \n",
    "        - Type: bool\n",
    "        - Description: A flag indicating whether to include all results or only those not used during training. \n",
    "                       If `True`, it includes all results for evaluation. If `False`, it excludes training images \n",
    "                       to focus only on non-training data.\n",
    "    \n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It generates a CSV file containing the evaluation \n",
    "                       results and saves it in the appropriate results folder.\n",
    "    \"\"\"\n",
    "\n",
    "    results_folder = get_results_folder(yolo_model_folder, img_dataset_folder)\n",
    "    labels = get_labels(os.path.join(yolo_model_folder, 'labels.txt'))\n",
    "\n",
    "    prediction_folder = os.path.join(results_folder, 'labels')\n",
    "    predictions_files = get_files(prediction_folder, 'txt')\n",
    "\n",
    "    correction_folder = os.path.join(results_folder, 'correctedLabels')\n",
    "    corrected_files = get_files(correction_folder, 'txt')\n",
    "\n",
    "    if not all_results:\n",
    "        output_file = os.path.join(results_folder, 'results/results_for_evaluation.csv')\n",
    "        img_use_for_training = get_img_from_training(yolo_model_folder, img_dataset_folder)\n",
    "        predictions_files = exclude_training_images(predictions_files, img_use_for_training)\n",
    "        corrected_files = exclude_training_images(corrected_files, img_use_for_training)\n",
    "    else:\n",
    "        output_file = os.path.join(results_folder, 'results/results_for_evaluation.csv')\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    pred_map = {os.path.basename(path): path for path in predictions_files}\n",
    "    corr_map = {os.path.basename(path): path for path in corrected_files}\n",
    "    \n",
    "    \n",
    "    # Browse through all the predictions\n",
    "    for basename, pred_path in pred_map.items():\n",
    "        # Retrieve the correction file if it exists\n",
    "        corr_path = corr_map.get(basename)\n",
    "     \n",
    "        if corr_path:\n",
    "            predictions = sorted(load_data_from_files([pred_file]), key=lambda x: (float(x.split(\" \")[1]), float(x.split()[2])))\n",
    "            corrections = sorted(load_data_from_files(matching_corr_files), key=lambda x: (float(x.split(\" \")[1]), float(x.split()[2])))\n",
    "            best_matches = get_best_iou_matches(predictions, corrections)\n",
    "\n",
    "            for prediction, best_correction, best_iou in best_matches:\n",
    "                pred_box = list(map(float, prediction.split(\" \")))\n",
    "                cls_pred = int(pred_box[0])\n",
    "                cls_corr = int(best_correction.split(\" \")[0])\n",
    "\n",
    "                if best_iou >= 0.5 and cls_pred == cls_corr:\n",
    "                    tp_fp_fn = 'TP'\n",
    "                elif best_iou >= 0.75 and cls_pred != cls_corr:\n",
    "                    tp_fp_fn = 'FP_class'\n",
    "                else:\n",
    "                    tp_fp_fn = 'FP'\n",
    "\n",
    "                if tp_fp_fn == 'FP':\n",
    "                    rows.append({\n",
    "                    'Filename': basename,\n",
    "                    'Predicted_coordinates': ', '.join(map(str, pred_box)),\n",
    "                    'Predicted_class': get_class_name(str(cls_pred), labels),\n",
    "                    'TP/FP/FN': tp_fp_fn,\n",
    "                    'cls_corr': '',\n",
    "                    'Corrected_coordinates': '',\n",
    "                    'IoU': '',\n",
    "                    'Confidence_score': pred_box[5] if len(pred_box) > 5 else None\n",
    "                })\n",
    "                else: \n",
    "                    rows.append({\n",
    "                        'Filename': basename,\n",
    "                        'Predicted_coordinates': ', '.join(map(str, pred_box)),\n",
    "                        'Predicted_class': get_class_name(str(cls_pred), labels),\n",
    "                        'TP/FP/FN': tp_fp_fn,\n",
    "                        'cls_corr': get_class_name(str(cls_corr), labels),\n",
    "                        'Corrected_coordinates': best_correction,\n",
    "                        'IoU': best_iou,\n",
    "                        'Confidence_score': pred_box[5] if len(pred_box) > 5 else None\n",
    "                    })\n",
    "    \n",
    "            matched_corrs = {c for _, c, _ in best_matches}\n",
    "            for corr in corrections:\n",
    "                if corr not in matched_corrs:\n",
    "                    box_corr = list(map(float, unmatched_correction.split(\" \")))\n",
    "                    cls_corr = int(box_corr[0])\n",
    "                    rows.append({\n",
    "                        'Filename': basename,\n",
    "                        'Predicted_coordinates': '',\n",
    "                        'Predicted_class': '',\n",
    "                        'TP/FP/FN': 'FN',\n",
    "                        'cls_corr': get_class_name(str(cls_corr), labels),\n",
    "                        'Corrected_coordinates': ', '.join(map(str, box_corr)),\n",
    "                        'IoU': 0,\n",
    "                        'Confidence_score': 0\n",
    "                    })\n",
    "\n",
    "        else:\n",
    "            # No correction file at all → all predictions can be considered FP\n",
    "            predictions = load_data_from_files([pred_path])\n",
    "            for pred in predictions:\n",
    "                box = list(map(float, pred.split()))\n",
    "                cls = int(box[0])\n",
    "                rows.append({\n",
    "                    'Filename': basename,\n",
    "                    'Predicted_coordinates': ', '.join(map(str, box)),\n",
    "                    'Predicted_class': get_class_name(str(cls), labels),\n",
    "                    'TP/FP/FN': 'FP',\n",
    "                    'Corrected_class': '',\n",
    "                    'Corrected_coordinates': '',\n",
    "                    'IoU': '',\n",
    "                    'Confidence_score': box[5] if len(box) > 5 else None\n",
    "                })\n",
    "    # Process *orphan* corrections (without associated predictions)\n",
    "    for basename, corr_path in corr_map.items():\n",
    "        if basename not in pred_map:\n",
    "            corrections = load_data_from_files([corr_path])\n",
    "            for corr in corrections:\n",
    "                box = list(map(float, corr.split()))\n",
    "                cls = int(box[0])\n",
    "                rows.append({\n",
    "                    'Filename': basename,\n",
    "                    'Predicted_coordinates': '',\n",
    "                    'Predicted_class': '',\n",
    "                    'TP/FP/FN': 'FN',\n",
    "                    'Corrected_class': get_class_name(str(cls), labels),\n",
    "                    'Corrected_coordinates': ', '.join(map(str, box)),\n",
    "                    'IoU': 0,\n",
    "                    'Confidence_score': 0\n",
    "                })\n",
    "    \n",
    "    save_results_to_csv(rows, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a4596-4a6b-4493-8efd-743b2384a681",
   "metadata": {},
   "source": [
    "## Get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fb235bcb-ac9a-418f-b5cf-21b3c4aae8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformated_decimal(tp, fp, fn, recall_class, precision_class, f1_score_class):\n",
    "    \"\"\"\n",
    "    This function formats the values of recall, precision, and F1 score to a consistent number of decimal places \n",
    "    based on the length of the input values for True Positives (TP), False Positives (FP), and False Negatives (FN).\n",
    "    \n",
    "    :param tp: \n",
    "        - Type: int\n",
    "        - Description: The number of True Positives (TP) for a given class.\n",
    "    :param fp: \n",
    "        - Type: int\n",
    "        - Description: The number of False Positives (FP) for a given class.\n",
    "    :param fn: \n",
    "        - Type: int\n",
    "        - Description: The number of False Negatives (FN) for a given class.\n",
    "    :param recall_class: \n",
    "        - Type: float\n",
    "        - Description: The recall value for the class, calculated as `TP / (TP + FN)`.\n",
    "    :param precision_class: \n",
    "        - Type: float\n",
    "        - Description: The precision value for the class, calculated as `TP / (TP + FP)`.\n",
    "    :param f1_score_class: \n",
    "        - Type: float\n",
    "        - Description: The F1 score value for the class, calculated as `2 * (precision * recall) / (precision + recall)`.\n",
    "    \n",
    "    :return: \n",
    "        - Type: tuple of str\n",
    "        - Description: Returns a tuple containing the formatted values for recall, precision, and F1 score, \n",
    "                       with a consistent number of decimal places based on the length of the input values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Establish the number of decimal places to use for formatting\n",
    "    tp_len = len(str(tp)) if len(str(tp)) > 0 else 0\n",
    "    fp_len = len(str(fp)) if len(str(fp)) > 0 else 0\n",
    "    fn_len= len(str(fn)) if len(str(fn)) > 0 else 0\n",
    "    max_decimal = max(tp_len, fp_len, fn_len)\n",
    "        \n",
    "    max_decimal += 1\n",
    "\n",
    "    # Ensure that the results are displayed consistently\n",
    "    recall_formated = \"{:.{}f}\".format(recall_class, max_decimal)\n",
    "    precision_formated = \"{:.{}f}\".format(precision_class, max_decimal)\n",
    "    f1_score_formated = \"{:.{}f}\".format(f1_score_class, max_decimal)\n",
    "    \n",
    "    return recall_formated, precision_formated, f1_score_formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "664ba9d7-abbf-448e-a03b-107eef85455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_results(img_dataset_folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function generates a text file and a PNG file summarizing the evaluation results of model predictions. \n",
    "    It processes a CSV file containing the previously generated statistics, calculates metrics such as recall, precision, \n",
    "    and F1 score for each class, and saves the results in a `.txt` file. A visual summary of the results is also saved \n",
    "    as a `.png` file.\n",
    "    \n",
    "    :param img_dataset_folder: \n",
    "        - Type: str\n",
    "        - Description: The path to the folder containing the dataset images. This folder is used to locate the \n",
    "                       corresponding results and statistics CSV file.\n",
    "    \n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It generates a `.txt` file with a summary of evaluation \n",
    "                       results and a `.png` file with a table displaying the calculated metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    results_folder = get_results_folder(yolo_model_folder, img_dataset_folder)\n",
    "    csv_with_results = os.path.join(results_folder, 'results/results_for_evaluation.csv')\n",
    " \n",
    "    df = pd.read_csv(csv_with_results, sep=';')\n",
    "    output_file = csv_with_results.replace('.csv', '.txt')\n",
    "    output_png = csv_with_results.replace('.csv', '.png')\n",
    "    \n",
    "    # Collect all unique classes present in the DataFrame\n",
    "    all_classes = np.unique(np.concatenate([df['Predicted_class'].dropna().unique(), df['Corrected_class'].dropna().unique()]))\n",
    "    print(f'Classes : {all_classes}')\n",
    " \n",
    "    table_data = []\n",
    " \n",
    "    # Initialize TP, FP and FN counters for all classes\n",
    "    class_TP = {classe: 0 for classe in all_classes}\n",
    "    class_FP = {classe: 0 for classe in all_classes}\n",
    "    class_FN = {classe: 0 for classe in all_classes}\n",
    " \n",
    "    # Browse DataFrame rows\n",
    "    for _, row in df.iterrows():\n",
    "        pred_class = row['Predicted_class']\n",
    "        corr_class = row['Corrected_class']\n",
    "        # Check TP/FP/FN status for line\n",
    "        if row['TP/FP/FN'] == 'TP':\n",
    "            class_TP[pred_class] += 1\n",
    "        elif row['TP/FP/FN'] == 'FP':\n",
    "            class_FP[pred_class] += 1\n",
    "        elif row['TP/FP/FN'] == 'FN':\n",
    "            class_FN[corr_class] += 1\n",
    "        elif row['TP/FP/FN'] == 'FP_class':\n",
    "            class_FN[corr_class] += 1\n",
    "            class_FP[pred_class] += 1\n",
    "            \n",
    " \n",
    "    # Calculate global totals\n",
    "    total_TP = sum(class_TP.values())\n",
    "    total_FP = sum(class_FP.values())\n",
    "    total_FN = sum(class_FN.values())\n",
    "    total_support = total_TP + total_FN\n",
    " \n",
    "    # Recall computation\n",
    "    recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) != 0 else 0\n",
    " \n",
    "    # Precision computation\n",
    "    precision = total_TP / (total_TP + total_FP) if(total_TP + total_FP) != 0 else 0\n",
    " \n",
    "    # Calculation of the overall F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    " \n",
    "    # Open the file in write mode\n",
    "    with open(output_file, 'w') as file:\n",
    "        # Écrire les résultats globaux\n",
    "        file.write(\"Overall results :\\n\")\n",
    "        file.write(\"Number of TP: {}\\n\".format(total_TP))\n",
    "        file.write(\"Number of FP : {}\\n\".format(total_FP))\n",
    "        file.write(\"Number of FN: {}\\n\".format(total_FN))\n",
    "        file.write(\"Recall (Recall) : {}\\n\".format(recall))\n",
    "        file.write(\"Precision : {}\\n\".format(precision))\n",
    "        file.write(\"Score F1 global : {}\\n\".format(f1_score))\n",
    "        file.write(f\"Support : {total_support}\\n\")\n",
    "        file.write(\"\\n\")\n",
    " \n",
    "        # Write results by class\n",
    "        file.write(\"Results per class :\\n\")\n",
    "        for classe in all_classes:\n",
    "            tp = class_TP[classe]\n",
    "            fp = class_FP[classe]\n",
    "            fn = class_FN[classe]\n",
    "            support = tp + fn\n",
    " \n",
    "            recall_class = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "            precision_class = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "            f1_score_class = 2 * (precision_class * recall_class) / (precision_class + recall_class) if (precision_class + recall_class) != 0 else 0\n",
    "            \n",
    "            recall_formated, precision_formated, f1_score_formated = reformated_decimal(tp, fp, fn, recall_class, precision_class, f1_score_class)\n",
    "            table_data.append([classe, tp, fp, fn, precision_formated, recall_formated, f1_score_formated, support])\n",
    "            \n",
    "            \n",
    "            file.write(\"Class {}\\n\".format(classe))\n",
    "            file.write(\"Number of TP: {}\\n\".format(tp))\n",
    "            file.write(\"Number of FP : {}\\n\".format(fp))\n",
    "            file.write(\"Number of FN: {}\\n\".format(fn))\n",
    "            file.write(\"Recall (Recall): {}\\n\".format(recall_class))\n",
    "            file.write(\"Precision : {}\\n\".format(precision_class))\n",
    "            file.write(\"Score F1 : {}\\n\".format(f1_score_class))\n",
    "            file.write(f\"Support : {total_support}\\n\")\n",
    "            file.write(\"\\n\")\n",
    " \n",
    "    print(f\"The {output_file} file has been created.\")\n",
    "\n",
    "    recall_formated, precision_formated, f1_score_formated = reformated_decimal(total_TP, total_FP, total_FN, recall, precision, f1_score)\n",
    "    table_data.append(['Overall', total_TP, total_FP, total_FN, precision_formated, recall_formated, f1_score_formated, total_support])\n",
    "    \n",
    "    # Generate a PNG file with a table\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    \n",
    "    table = ax.table(cellText=table_data, colLabels=['Classes', 'Nb TP', 'Nb FP', 'Nb FN', 'Precision', 'Rappel', 'Score F1', 'Support'],\n",
    "                     loc='center', cellLoc='center')\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.auto_set_column_width([0, 1, 2, 3, 4, 5, 6,7])\n",
    "    \n",
    "    plt.savefig(output_png, bbox_inches='tight')\n",
    "    plt.show()\n",
    " \n",
    "    print(f\"The {output_png} file has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f322b-691c-49b2-bf57-37699b46c124",
   "metadata": {},
   "source": [
    "## Print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e0640e00-89b3-4a4c-b2b2-e7654fc5853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(img_dataset_folder, yolo_model_folder):\n",
    "    \"\"\"\n",
    "    This function generates a confusion matrix to evaluate the performance and robustness of a trained YOLO model. \n",
    "    The matrix is created by comparing the predicted classes with the corrected (ground truth) classes, and the \n",
    "    result is visualized as an image.\n",
    "    \n",
    "    :param img_dataset_folder: \n",
    "        - Type: str\n",
    "        - Description: The path to the folder containing the dataset images. This folder is used to locate \n",
    "                       the corresponding results and statistics CSV file.\n",
    "    :param yolo_model_folder: \n",
    "        - Type: str\n",
    "        - Description: The path to the folder containing the YOLO model and its associated data. This folder is \n",
    "                       used to access the labels file and the results CSV file.\n",
    "    \n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It generates a confusion matrix image and saves it \n",
    "                       in the results folder for easy visualization.\n",
    "    \n",
    "    The confusion matrix is displayed using a heatmap format, and the resulting image is saved in the results folder.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    results_folder = get_results_folder(yolo_model_folder, img_dataset_folder)\n",
    "    csv_with_results  = os.path.join(results_folder, 'results/results_for_evaluation.csv')\n",
    "\n",
    "    # Load labels from the labels file\n",
    "    labels = get_labels(os.path.join(yolo_model_folder, 'labels.txt'))\n",
    "    display_labels=list(labels.values())\n",
    "    \n",
    "    # Add the 'Background' class used for file the NaN results\n",
    "    display_labels.append('Background')\n",
    "    \n",
    "    # Open the csv with results\n",
    "    results = pd.read_csv(csv_with_results, sep=';')\n",
    "\n",
    "    # Replace the NaN results with 'Background', the class will be used to show the FP and FN\n",
    "    predictions = results['Predicted_class'].fillna('Background')\n",
    "    corrections = results['Corrected_class'].fillna('Background')\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    confusion_matix = metrics.confusion_matrix(y_pred=predictions, y_true=corrections, labels=display_labels)\n",
    "    # print(confusion_matix)\n",
    "\n",
    "    # To create a more interpretable visual display we need to convert the table into a confusion matrix display\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matix, display_labels=display_labels)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Costumizing and visualizing the display with rotation of x-axis labels\n",
    "    cm_display.plot(ax=ax, xticks_rotation=90, cmap='autumn', values_format='d')\n",
    "\n",
    "    plt.title('Confusion matrice')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_folder, 'results','confusionMatrice.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5348ff-9707-4a5e-a48a-0ddaf20cd901",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ef067-0373-4914-8f70-75461c1debc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_dataset_folder = 'ABSPATHTOTHEFOLDER' # to be changed, asbolute path to a folder with images only, without annotations.\n",
    "yolo_model_folder = 'ABSPATHTOTHEMODELFOLDER' # to be changed, asbolute path to the folder with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43e3e8-8b98-414e-8279-26b81465c88b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the corrected files in YOLO format\n",
    "get_corrected_label_files(img_dataset_folder, yolo_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0c483-7322-4d30-9fa1-232dcb27b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a CSV with the corrected data\n",
    "get_csv_results(img_dataset_folder, yolo_model_folder, all_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6848544c-9373-4045-9929-e443bcb26ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the file with metrics\n",
    "get_txt_results(img_dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc2c9c-b607-4785-a686-965272901f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "create_confusion_matrix(img_dataset_folder, yolo_model_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiamat_env",
   "language": "python",
   "name": "tiamat_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
