{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aac7f8d-f982-4ac8-835c-fd4cbb383f78",
   "metadata": {},
   "source": [
    "# Correction from Label Studio and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601803b8-ff63-49ce-8ed0-94eea362e577",
   "metadata": {},
   "source": [
    "This notebook SHOULD NOT be launched until any corrections have been made in Label Studio.\n",
    "The purpose of the various functions is to be able to extract the data to be analysed after training in order to assess the robustness of the model.\n",
    "\n",
    "**Warning 1**\n",
    "The requirement is to use, or create, a Label Studio account: (https://labelstud.io/guide/install.html) and to have followed the workflow of the previous notebooks or at least to have created similar files.\n",
    "\n",
    "**Warning 2**\n",
    "The export format for corrections from Label Studio MUST be csv ONLY.\n",
    "Label Studio's YOLO export format does not allow you to keep the names of the images as they were imported:  in the context of this workflow, these are the URLs of the images from which this script can retrieve the name of the image and generate .txt files with the same name as the image (thus allowing you to use this new data for a new training session).\n",
    "\n",
    "**Warning 3**\n",
    "It is essential to complete the \"Labeling Interface\" by specifying strictly the same class names (case, special characters, etc.) as those declared in the .json file.\n",
    "\n",
    "**Warning 4**\n",
    "If there is some issue in Label Studio, you can change the labeling results' name from \"predictions\" to \"annotations\", since Label documentation explain that you can't change the bounding box coordinates for prediction, only for detection : https://labelstud.io/guide/predictions.html#Predictions-are-read-only.\n",
    "But I tried and you can change the predicted bounding boxes and export the new data with the changes without any problem.\n",
    "\n",
    "**Notice concerning use** \n",
    "Any use, even partial, of the content of this notebook must be accompanied by an appropriate citation.\n",
    "\n",
    "&copy; 2023 Marion Charpier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ab46e-2d24-49d9-853a-5f6d136ecb5d",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f71f0ae-921e-42db-a318-1061a5c2afe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent / 'modules'))\n",
    "\n",
    "from folders_path import *\n",
    "from transform_coordinates_functions import from_ls_to_yolo\n",
    "from class_names_functions import get_labels, get_class_name, get_class_code\n",
    "from manipulate_files import open_json_file, save_json_file, get_files, exclude_training_images, load_data_from_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488719b-87fa-4946-99a7-0bee1799580e",
   "metadata": {},
   "source": [
    "## Get the Label Studio correction in new .txt files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401ea06",
   "metadata": {},
   "source": [
    "### Update YOLO label definitions with new classes found in prediction correction files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "197f346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_labels(project_folder:str, yolo_model_folder:str) -> None:\n",
    "    \"\"\"\n",
    "    Updates the YOLO labels file with new classes found in manually corrected prediction files.\n",
    "\n",
    "    If new classes are detected in the correction JSON files that are not already listed in\n",
    "    the model's labels.txt, they are added with new IDs. The updated labels file is saved\n",
    "    to the results folder. If no new classes are found, the original file is simply copied.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    project_folder : str\n",
    "        Path to the main project directory.\n",
    "\n",
    "    yolo_model_folder : str\n",
    "        Path to the folder containing the trained YOLO model and its labels.txt file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        A new labels.txt file is saved in the results folder.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load existing labels (may be a dict {\"0\":\"label\"} or a list [\"label\"])\n",
    "    labels_file = Path(yolo_model_folder) / 'labels.txt'\n",
    "    labels = get_labels(str(labels_file))\n",
    "    \n",
    "    # Get results folder (destination for corrected labels) and ensure it exists\n",
    "    results_folder = Path(get_results_folder(project_folder, yolo_model_folder))\n",
    "    results_folder.mkdir(parents=True, exist_ok=True)\n",
    "    label_dict_file = results_folder / 'labels.txt'\n",
    "\n",
    "    # Normalize labels to a dictionary {id: name}\n",
    "    if isinstance(labels, dict):\n",
    "        train_labels = dict(labels)\n",
    "    else:\n",
    "        train_labels = {str(i): name for i, name in enumerate(labels)}\n",
    "    existing_values = set(train_labels.values())\n",
    "\n",
    "    # Folder containing manual correction files\n",
    "    corrections_folder = Path(get_corrections_folder_inference(project_folder)) \n",
    "    correction_files = [f for f in corrections_folder.iterdir() if not f.name.startswith('.')]\n",
    "    \n",
    "    unique_classes = set()\n",
    "    \n",
    "    # Extract all unique corrected classes from correction file\n",
    "    for correction_file in correction_files:\n",
    "        corrections = open_json_file(str(correction_file))\n",
    "        \n",
    "        for i, result in enumerate(corrections['result']):\n",
    "            value = result.get('value', {})\n",
    "            labels_ls = value.get('rectanglelabels', [])\n",
    "            if labels_ls:\n",
    "                unique_classes.add(labels_ls[0])\n",
    "\n",
    "    corrected_classes = list(unique_classes)\n",
    "    \n",
    "    # Identify labels that are not already in the training set\n",
    "    new_labels = [c for c in corrected_classes if c not in existing_values]\n",
    "    \n",
    "    if new_labels:\n",
    "        print(f\"{len(new_labels)} new label(s) found in the correction files: {new_labels}\")\n",
    "        \n",
    "        # Assign new incremental IDs starting after the last current ID\n",
    "        max_id = max(map(int, train_labels.keys())) if train_labels else -1\n",
    "        for i, cls, in enumerate(new_labels, start=max_id+1):\n",
    "            train_labels[str(i)] = cls\n",
    "        \n",
    "        # Write the updated labels file\n",
    "        with open(label_dict_file, \"w\", encoding='utf-8') as f:\n",
    "            for k, v in train_labels.items():\n",
    "                f.write(f\"'{k}': '{v}'\\n\")\n",
    "        print(f\"Labels file written in {label_dict_file} \")\n",
    "    \n",
    "    else:\n",
    "        # No new labels → copy the existing file\n",
    "        shutil.copy2 (labels_file, label_dict_file)\n",
    "        print(f\"No new class found. Labels file copied to {label_dict_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac47a6-af7d-4dea-ab15-45cebe69157d",
   "metadata": {},
   "source": [
    "### Generate new txt files with correct bounding boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73613c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrected_label_files(project_folder:str, yolo_model_folder:str) -> None:\n",
    "    \"\"\"\n",
    "    Converts corrected annotation files (Label Studio format) into YOLOv8-compatible .txt files.\n",
    "\n",
    "    This function processes all JSON files containing manually corrected annotations. It removes\n",
    "    confidence scores, skips deleted annotation boxes, and writes new YOLO-format label files for\n",
    "    each image. The output files are saved in the `correctedLabels` folder under the results directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    project_folder : str\n",
    "        Path to the project directory containing the corrected JSON files.\n",
    "\n",
    "    yolo_model_folder : str\n",
    "        Path to the YOLO model folder containing the 'labels.txt' file for class mapping.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        YOLO-format annotation files are saved in:\n",
    "        `<runs/predict/<project>_<model>/correctedLabels>`.\n",
    "    \"\"\"\n",
    "    \n",
    "    corrections_folder = Path(get_corrections_folder_inference(project_folder))\n",
    "    results_folder = Path(get_results_folder(project_folder, yolo_model_folder))\n",
    "    \n",
    "    label_dict_file = results_folder / 'labels.txt'\n",
    "    labels = get_labels(label_dict_file)\n",
    "    \n",
    "    label_dict_folder = Path(get_correctedLabels_folder(project_folder, yolo_model_folder))\n",
    "    label_dict_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "\n",
    "    # Retrieve corrected JSON files as a list and open them\n",
    "    corrected_files = [f for f in corrections_folder.iterdir() if not f.name.startswith('.')]\n",
    "    \n",
    "    for corrected_file in corrected_files:\n",
    "        corrections = open_json_file(corrected_file)\n",
    "\n",
    "        for result_item in corrections['result']:\n",
    "            result_item.pop('score', None)\n",
    "        save_json_file(corrected_file, corrections)\n",
    "        \n",
    "        # Retrieve image name from corrected annotations file\n",
    "        name = corrections['task']['data']['image']\n",
    "        img_name = Path(name).stem\n",
    "        result = corrections['result']\n",
    "        \n",
    "\n",
    "        # Create a .txt file with annotation data\n",
    "        with open(label_dict_folder / f\"{img_name}.txt\", 'w') as yolo_correction:\n",
    "            for item in result:\n",
    "                if \"id\" not in item:\n",
    "                    #Skipped (deleted box)\n",
    "                    print(\"Prediction box erased.\")\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                # Retrieve annotation box coordinates\n",
    "                value = item['value']\n",
    "                x, y, w, h = from_ls_to_yolo(value['x'], value['y'], value['width'], value['height'])\n",
    "\n",
    "                # Retrieve the annotation label and associate it with its number in the \"labels.txt\" file\n",
    "                class_name = value['rectanglelabels'][0]\n",
    "                class_id = get_class_code(class_name, labels)\n",
    "                \n",
    "                yolo_correction.write(f\"{class_id} {x} {y} {w} {h}\\n\")\n",
    "\n",
    "    print(f\"✅ All corrected annotations have been converted to YOLO format in: {label_dict_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41ad14-38fd-4fab-96f8-84b74dca97b1",
   "metadata": {},
   "source": [
    "## Get results in csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b345c-c591-4a41-a0ea-20f2841e93e5",
   "metadata": {},
   "source": [
    "### Get a list of images used for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e88d4d76-793c-4428-92d6-fbf901bb7de5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_from_training(project_folder:str, yolo_model_folder:str) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of images from the dataset folder that were used during the YOLO model training.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    project_folder : str\n",
    "\n",
    "    yolo_model_folder : str\n",
    "        Path to the YOLO model folder (should contain 'dataset_statistics/training_dataset.txt').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of image filenames that were used for training.\n",
    "    \"\"\"\n",
    "\n",
    "    yolo_model_folder = Path(yolo_model_folder)\n",
    "    \n",
    "    training_dataset = yolo_model_folder / 'dataset_statistics'/ 'training_dataset.txt'\n",
    "    training_folder = Path(get_img_folder_training(project_folder))\n",
    "    \n",
    "    if not training_dataset.exists():\n",
    "        raise FileNotFoundError(f\"Training dataset file not found: {training_dataset}\")\n",
    "    \n",
    "    with open(training_dataset, 'r') as train_data_file:\n",
    "            train_image_names = [Path(line.strip()).name for line in train_data_file if line.strip()]\n",
    "    \n",
    "    img_exts = {'.jpg', '.jpeg', '.png', '.tiff'}  \n",
    "    image_files = [file.name for file in training_folder.iterdir() if file.suffix.lower() in img_exts]\n",
    "    \n",
    "    matching_images = [image_name for image_name in train_image_names if Path(image_name).name in image_files]\n",
    "   \n",
    "    if matching_images:\n",
    "        print(\"✅ The following images were used to train the model:\")\n",
    "        for img in matching_images:\n",
    "            print(f\" - {img}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"⚠️ No matching images found in {training_folder} for the model {yolo_model_folder.name}.\")\n",
    "    \n",
    "    return matching_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dce9be-022f-49f1-b043-e2841fba7fc5",
   "metadata": {},
   "source": [
    "### Calculate IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54ca6562-b757-4b62-abf2-e570848b7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1:list, box2:list) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the Intersection over Union (IoU) between two bounding boxes. IoU is a measure \n",
    "    of the overlap between two bounding boxes and is commonly used to evaluate the accuracy of object detection models.\n",
    "\n",
    "    The function is adapted from the 'bb_intersection_over_union' function on PyImageSearch, which uses \n",
    "    bounding box coordinates in (x_min, y_min, x_max, y_max) format. The adaptation accounts for the \n",
    "    fact that YOLOv8 provides bounding box coordinates in relative format (x_center, y_center, width, height).\n",
    "\n",
    "    :param box1: \n",
    "        - Type: list or tuple\n",
    "        - Description: The first bounding box defined as a list or tuple of values [class_id, x_center, y_center, width, height]. \n",
    "                       The coordinates are relative to the image dimensions.\n",
    "    :param box2: \n",
    "        - Type: list or tuple\n",
    "        - Description: The second bounding box defined as a list or tuple of values [class_id, x_center, y_center, width, height]. \n",
    "                       The coordinates are relative to the image dimensions.\n",
    "    \n",
    "    :return: \n",
    "        - Type: float\n",
    "        - Description: The IoU value, which ranges from 0 to 1. A value of 0 indicates no overlap, \n",
    "                       while a value of 1 indicates perfect overlap between the two bounding boxes.\n",
    "\n",
    "    This function is useful for evaluating object detection models and determining how well the predicted bounding boxes \n",
    "    match the ground truth annotations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert coordinates (x, y, w, h) in (x_min, y_min, x_max, y_max)\n",
    "    box1_x_min = box1[1] - box1[3] / 2\n",
    "    box1_y_min = box1[2] - box1[4] / 2\n",
    "    box1_x_max = box1[1] + box1[3] / 2\n",
    "    box1_y_max = box1[2] + box1[4] / 2\n",
    "    \n",
    "    box2_x_min = box2[1] - box2[3] / 2\n",
    "    box2_y_min = box2[2] - box2[4] / 2\n",
    "    box2_x_max = box2[1] + box2[3] / 2\n",
    "    box2_y_max = box2[2] + box2[4] / 2\n",
    "    \n",
    "    # Calculate coordinates (x,y) of the overlap\n",
    "    x_min = max(box1_x_min, box2_x_min)\n",
    "    y_min = max(box1_y_min, box2_y_min)\n",
    "    x_max = min(box1_x_max, box2_x_max)\n",
    "    y_max = min(box1_y_max, box2_y_max)\n",
    "    \n",
    "    # Calculate the area of the overlap\n",
    "    intersection_area = max(0, x_max - x_min + 1) * max(0, y_max - y_min + 1)\n",
    "\n",
    "    # Calculer the area of the two bounding boxes\n",
    "    box1_area = (box1_x_max - box1_x_min + 1) * (box1_y_max - box1_y_min + 1)\n",
    "    box2_area = (box2_x_max - box2_x_min + 1) * (box2_y_max - box2_y_min + 1)\n",
    "    \n",
    "    # Calculate the Intersection over Union (IoU)\n",
    "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a72d7-cbbd-40cd-b7b1-ca391a8aa2ff",
   "metadata": {},
   "source": [
    "### Get the best iou macthes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0eb7d1-a69d-41ee-b422-a20d1f37e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_iou_matches(predictions:list, corrected_predictions:list) -> list:\n",
    "    \"\"\"\n",
    "    This function finds the best matching corrected bounding box for each predicted bounding box based on \n",
    "    the Intersection over Union (IoU) value. For each prediction, it calculates the IoU with all corrected \n",
    "    bounding boxes and selects the one with the highest IoU as the best match.\n",
    "    \n",
    "    :param predictions: \n",
    "        - Type: list of str\n",
    "        - Description: A list of predicted bounding boxes in YOLO format (class_id, x_center, y_center, width, height). \n",
    "                       Each bounding box is represented as a string of space-separated values.\n",
    "    :param corrected_predictions: \n",
    "        - Type: list of str\n",
    "        - Description: A list of corrected bounding boxes in YOLO format (class_id, x_center, y_center, width, height). \n",
    "                       Each bounding box is represented as a string of space-separated values.\n",
    "    \n",
    "    :return: \n",
    "        - Type: list of tuples\n",
    "        - Description: A list of tuples, where each tuple contains:\n",
    "            - The predicted bounding box (str)\n",
    "            - The best matching corrected bounding box (str) based on the highest IoU\n",
    "            - The IoU value (float) for the best match\n",
    "    \n",
    "    This function is useful for evaluating the performance of a model by comparing its predictions with manually corrected \n",
    "    ground truth annotations, identifying the best matches based on spatial overlap.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an empty list for the best matches\n",
    "    best_matches = []\n",
    "\n",
    "    for prediction in predictions:\n",
    "        prediction_box = prediction.split()\n",
    "        prediction_box = [float(coord) for coord in prediction_box]\n",
    "        best_iou = 0\n",
    "        best_correction = None\n",
    "\n",
    "        for correction in corrected_predictions:\n",
    "            correction_box = correction.split()\n",
    "            correction_box = [float(coord) for coord in correction_box]\n",
    "\n",
    "            iou = calculate_iou(prediction_box, correction_box)\n",
    "            \n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_correction = correction\n",
    "        \n",
    "        best_matches.append((prediction, best_correction, best_iou))\n",
    "    \n",
    "    return best_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f499f0-b1d4-4a7c-afea-ee0ab341cd4a",
   "metadata": {},
   "source": [
    "### Load data from files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb5167-9dbd-473b-93fb-ecfb8c6629e4",
   "metadata": {},
   "source": [
    "### Save results in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f470247f-556b-4125-82ca-6826eb3b3fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(rows:list, output_file:str) -> None:\n",
    "    \"\"\"\n",
    "    This function saves a list of generated and corrected annotations into a CSV file. If no annotations are provided, \n",
    "    it logs a message indicating that no corrections were made and exits the function. Otherwise, it creates a \n",
    "    DataFrame from the provided data, sorts it by the 'Filename' column, and writes it to the specified CSV file.\n",
    "    \n",
    "    :param rows: \n",
    "        - Type: list of dict\n",
    "        - Description: A list of dictionaries containing the generated and corrected annotations. Each dictionary should \n",
    "                       represent a single annotation entry with keys as column names.\n",
    "    :param output_file: \n",
    "        - Type: str\n",
    "        - Description: The path where the CSV file will be created. This file will store the sorted annotations for easy review.\n",
    "    \n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It either creates the CSV file or prints a message if no \n",
    "                       annotations are provided.\n",
    "    \n",
    "    This function is useful for storing annotation results in a structured format, facilitating further analysis or \n",
    "    review of corrected and generated annotations.\n",
    "    \"\"\"\n",
    "\n",
    "    if not rows:\n",
    "        print('No correction made')\n",
    "        return\n",
    "    df = pd.DataFrame(rows)\n",
    "    df_sorted = df.sort_values('Filename')\n",
    "    df_sorted.to_csv(output_file, sep=';',index=False)\n",
    "    print(f\"The {output_file} file has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2313e7c6-d84a-4e7f-b8d5-897b48fd2e60",
   "metadata": {},
   "source": [
    "### Generate the csv with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc007e-a496-4d5d-bcde-46b3d62c2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_results(project_folder:str, yolo_model_folder:str, all_results:bool) -> None:\n",
    "    \"\"\"\n",
    "    Generate a CSV file summarizing the evaluation of YOLO model predictions against manually corrected annotations.\n",
    "\n",
    "    Each prediction is evaluated as:\n",
    "        - TP (True Positive): correct class and IoU ≥ 0.5\n",
    "        - FP (False Positive): incorrect or unmatched prediction\n",
    "        - FP_class: correct box but wrong class (IoU ≥ 0.75)\n",
    "        - FN (False Negative): missing prediction for a corrected annotation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    project_folder : str\n",
    "        Path to the project directory.\n",
    "    \n",
    "    yolo_model_folder : str\n",
    "        Path to the folder containing the YOLO model and its associated output (e.g. labels.txt, predictions).\n",
    "\n",
    "    all_results : bool\n",
    "        If True, evaluates all predictions.\n",
    "        If False, excludes predictions from images used during training (based on training_dataset.txt).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        The evaluation is saved as a CSV file in the results folder under 'results/results_for_evaluation.csv'.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Uses best IoU matching between predictions and corrected labels.\n",
    "    - Assumes YOLO annotations follow standard YOLO format (class x y w h confidence).\n",
    "    - Corrected labels are expected in 'correctedLabels' folder.\n",
    "    \"\"\"\n",
    "\n",
    "    results_folder = Path(get_results_folder(project_folder, yolo_model_folder))\n",
    "    label_dict = get_labels(str(results_folder / 'labels.txt'))\n",
    "\n",
    "    prediction_folder = results_folder / 'labels'\n",
    "    predictions_files = get_files(str(prediction_folder), 'txt')\n",
    "\n",
    "    correction_folder = results_folder / 'correctedLabels'\n",
    "    corrected_files = get_files(str(correction_folder), 'txt')\n",
    "\n",
    "    output_file = results_folder / 'results' / 'results_for_evaluation.csv'\n",
    "\n",
    "    if not all_results:\n",
    "        img_use_for_training = get_img_from_training(project_folder, yolo_model_folder)\n",
    "        predictions_files = exclude_training_images(predictions_files, img_use_for_training)\n",
    "        corrected_files = exclude_training_images(corrected_files, img_use_for_training)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    pred_map = {Path(path).name: Path(path) for path in predictions_files}\n",
    "    corr_map = {Path(path).name: Path(path) for path in corrected_files}\n",
    "    \n",
    "    \n",
    "    # Browse through all the predictions\n",
    "    for basename, pred_path in pred_map.items():\n",
    "        # Retrieve the correction file if it exists\n",
    "        corr_path = corr_map.get(basename)\n",
    "\n",
    "        # HIC SUNT DRACONES\n",
    "        if corr_path:\n",
    "            predictions = sorted(load_data_from_files([str(pred_path)]), key=lambda x: (float(x.split()[1]), float(x.split()[2])))\n",
    "            corrections = sorted(load_data_from_files([str(corr_path)]), key=lambda x: (float(x.split()[1]), float(x.split()[2])))\n",
    "            best_matches = get_best_iou_matches(predictions, corrections)\n",
    "\n",
    "            for prediction, best_correction, best_iou in best_matches:\n",
    "                pred_box = list(map(float, prediction.split()))\n",
    "                cls_pred = int(pred_box[0])\n",
    "                cls_corr = int(best_correction.split()[0])\n",
    "\n",
    "                if best_iou >= 0.5 and cls_pred == cls_corr:\n",
    "                    tp_fp_fn = 'TP'\n",
    "                # \n",
    "                elif best_iou >= 0.75 and cls_pred != cls_corr:\n",
    "                    tp_fp_fn = 'FP_class'\n",
    "                else:\n",
    "                    tp_fp_fn = 'FP'\n",
    "\n",
    "                if tp_fp_fn == 'FP':\n",
    "                    rows.append({\n",
    "                    'Filename': basename,\n",
    "                    'Predicted_coordinates': ', '.join(map(str, pred_box)),\n",
    "                    'Predicted_class': get_class_name(str(cls_pred), label_dict),\n",
    "                    'TP/FP/FN': tp_fp_fn,\n",
    "                    'Corrected_class': '',\n",
    "                    'Corrected_coordinates': '',\n",
    "                    'IoU': 0.0,\n",
    "                    'Confidence_score': pred_box[5] if len(pred_box) > 5 else 0.0\n",
    "                })\n",
    "                else: \n",
    "                    rows.append({\n",
    "                        'Filename': basename,\n",
    "                        'Predicted_coordinates': ', '.join(map(str, pred_box)),\n",
    "                        'Predicted_class': get_class_name(str(cls_pred), label_dict),\n",
    "                        'TP/FP/FN': tp_fp_fn,\n",
    "                        'Corrected_class': get_class_name(str(cls_corr), label_dict),\n",
    "                        'Corrected_coordinates': best_correction,\n",
    "                        'IoU': best_iou,\n",
    "                        'Confidence_score': pred_box[5] if len(pred_box) > 5 else 0.0\n",
    "                    })\n",
    "    \n",
    "            matched_corrs = {c for _, c, _ in best_matches}\n",
    "            for corr in corrections:\n",
    "                if corr not in matched_corrs:\n",
    "                    box_corr = list(map(float, corr.split()))\n",
    "                    cls_corr = int(box_corr[0])\n",
    "                    rows.append({\n",
    "                        'Filename': basename,\n",
    "                        'Predicted_coordinates': '',\n",
    "                        'Predicted_class': '',\n",
    "                        'TP/FP/FN': 'FN',\n",
    "                        'Corrected_class': get_class_name(str(cls_corr), label_dict),\n",
    "                        'Corrected_coordinates': ', '.join(map(str, box_corr)),\n",
    "                        'IoU': 0.0,\n",
    "                        'Confidence_score': 0.0\n",
    "                    })\n",
    "\n",
    "        else:\n",
    "            # No correction file at all → all predictions can be considered FP\n",
    "            predictions = load_data_from_files([pred_path])\n",
    "            for pred in predictions:\n",
    "                box = list(map(float, pred.split()))\n",
    "                cls = int(box[0])\n",
    "                rows.append({\n",
    "                    'Filename': basename,\n",
    "                    'Predicted_coordinates': ', '.join(map(str, box)),\n",
    "                    'Predicted_class': get_class_name(str(cls), label_dict),\n",
    "                    'TP/FP/FN': 'FP',\n",
    "                    'Corrected_class': '',\n",
    "                    'Corrected_coordinates': '',\n",
    "                    'IoU': 0.0,\n",
    "                    'Confidence_score': box[5] if len(box) > 5 else 0.0\n",
    "                })\n",
    "\n",
    "    # HIC SUNT DRACONES\n",
    "\n",
    "    # Process *orphan* corrections (without associated predictions)\n",
    "    for basename, corr_path in corr_map.items():\n",
    "        if basename not in pred_map:\n",
    "            corrections = load_data_from_files([corr_path])\n",
    "            for corr in corrections:\n",
    "                box = list(map(float, corr.split()))\n",
    "                cls = int(box[0])\n",
    "                rows.append({\n",
    "                    'Filename': basename,\n",
    "                    'Predicted_coordinates': '',\n",
    "                    'Predicted_class': '',\n",
    "                    'TP/FP/FN': 'FN',\n",
    "                    'Corrected_class': get_class_name(str(cls), label_dict),\n",
    "                    'Corrected_coordinates': ', '.join(map(str, box)),\n",
    "                    'IoU': 0.0,\n",
    "                    'Confidence_score': 0.0\n",
    "                })\n",
    "    \n",
    "    save_results_to_csv(rows, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a4596-4a6b-4493-8efd-743b2384a681",
   "metadata": {},
   "source": [
    "## Get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb235bcb-ac9a-418f-b5cf-21b3c4aae8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformated_decimal(tp:int, fp:int, fn:int, recall_class:float, precision_class:float, f1_score_class:float) -> tuple:\n",
    "    \"\"\"\n",
    "    This function formats the values of recall, precision, and F1 score to a consistent number of decimal places \n",
    "    based on the length of the input values for True Positives (TP), False Positives (FP), and False Negatives (FN).\n",
    "    \n",
    "    :param tp: \n",
    "        - Type: int\n",
    "        - Description: The number of True Positives (TP) for a given class.\n",
    "    :param fp: \n",
    "        - Type: int\n",
    "        - Description: The number of False Positives (FP) for a given class.\n",
    "    :param fn: \n",
    "        - Type: int\n",
    "        - Description: The number of False Negatives (FN) for a given class.\n",
    "    :param recall_class: \n",
    "        - Type: float\n",
    "        - Description: The recall value for the class, calculated as `TP / (TP + FN)`.\n",
    "    :param precision_class: \n",
    "        - Type: float\n",
    "        - Description: The precision value for the class, calculated as `TP / (TP + FP)`.\n",
    "    :param f1_score_class: \n",
    "        - Type: float\n",
    "        - Description: The F1 score value for the class, calculated as `2 * (precision * recall) / (precision + recall)`.\n",
    "    \n",
    "    :return: \n",
    "        - Type: tuple of str\n",
    "        - Description: Returns a tuple containing the formatted values for recall, precision, and F1 score, \n",
    "                       with a consistent number of decimal places based on the length of the input values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Establish the number of decimal places to use for formatting\n",
    "    tp_len = len(str(tp)) if len(str(tp)) > 0 else 0\n",
    "    fp_len = len(str(fp)) if len(str(fp)) > 0 else 0\n",
    "    fn_len= len(str(fn)) if len(str(fn)) > 0 else 0\n",
    "    max_decimal = max(tp_len, fp_len, fn_len)\n",
    "        \n",
    "    max_decimal += 1\n",
    "\n",
    "    # Ensure that the results are displayed consistently\n",
    "    recall_formated = \"{:.{}f}\".format(recall_class, max_decimal)\n",
    "    precision_formated = \"{:.{}f}\".format(precision_class, max_decimal)\n",
    "    f1_score_formated = \"{:.{}f}\".format(f1_score_class, max_decimal)\n",
    "    \n",
    "    return recall_formated, precision_formated, f1_score_formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "664ba9d7-abbf-448e-a03b-107eef85455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_results(project_folder:str, yolo_model_folder:str) -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a text summary and visual table (PNG) of evaluation metrics from YOLO predictions.\n",
    "\n",
    "    Metrics include TP, FP, FN, recall, precision, and F1-score, both globally and per class.\n",
    "    Data is sourced from a CSV file generated by the evaluation pipeline.\n",
    "\n",
    "    \n",
    "    :param project_folder: \n",
    "        - Type: str\n",
    "        - Description: Path to the project folder.\n",
    "    \n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It generates a `.txt` file with a summary of evaluation \n",
    "                       results and a `.png` file with a table displaying the calculated metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    results_folder = Path(get_results_folder(project_folder, yolo_model_folder))\n",
    "    csv_with_results = results_folder / 'results'/ 'results_for_evaluation.csv'\n",
    "    if not csv_with_results.exists():\n",
    "        raise FileNotFoundError(f\"No CSV found at {csv_with_results}\")\n",
    "\n",
    " \n",
    "    df = pd.read_csv(csv_with_results, sep=';')\n",
    "    output_file_txt = csv_with_results.with_suffix('.txt')\n",
    "    output_png = csv_with_results.with_suffix('.png')\n",
    "    \n",
    "    # Collect all unique classes present in the DataFrame\n",
    "    all_classes = np.unique(np.concatenate([df['Predicted_class'].dropna().unique(), df['Corrected_class'].dropna().unique()]))\n",
    "    print(f'Classes : {all_classes}')\n",
    " \n",
    "    table_data = []\n",
    " \n",
    "    # Initialize TP, FP and FN counters for all classes\n",
    "    class_TP = {classe: 0 for classe in all_classes}\n",
    "    class_FP = {classe: 0 for classe in all_classes}\n",
    "    class_FN = {classe: 0 for classe in all_classes}\n",
    " \n",
    "    # Browse DataFrame rows\n",
    "    for _, row in df.iterrows():\n",
    "        pred_class = row['Predicted_class']\n",
    "        corr_class = row['Corrected_class']\n",
    "        # Check TP/FP/FN status for line\n",
    "        if row['TP/FP/FN'] == 'TP':\n",
    "            class_TP[pred_class] += 1\n",
    "        elif row['TP/FP/FN'] == 'FP':\n",
    "            class_FP[pred_class] += 1\n",
    "        elif row['TP/FP/FN'] == 'FN':\n",
    "            class_FN[corr_class] += 1\n",
    "        elif row['TP/FP/FN'] == 'FP_class':\n",
    "            class_FN[corr_class] += 1\n",
    "            class_FP[pred_class] += 1\n",
    "            \n",
    " \n",
    "    # Calculate global totals\n",
    "    total_TP = sum(class_TP.values())\n",
    "    total_FP = sum(class_FP.values())\n",
    "    total_FN = sum(class_FN.values())\n",
    "    total_support = total_TP + total_FN\n",
    " \n",
    "    # Recall computation\n",
    "    recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) != 0 else 0\n",
    " \n",
    "    # Precision computation\n",
    "    precision = total_TP / (total_TP + total_FP) if(total_TP + total_FP) != 0 else 0\n",
    " \n",
    "    # Calculation of the overall F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    " \n",
    "    # Open the file in write mode\n",
    "    with open(output_file_txt, 'w') as file:\n",
    "        # Écrire les résultats globaux\n",
    "        file.write(\"Overall results :\\n\")\n",
    "        file.write(\"Number of TP: {}\\n\".format(total_TP))\n",
    "        file.write(\"Number of FP : {}\\n\".format(total_FP))\n",
    "        file.write(\"Number of FN: {}\\n\".format(total_FN))\n",
    "        file.write(\"Recall (Recall) : {}\\n\".format(recall))\n",
    "        file.write(\"Precision : {}\\n\".format(precision))\n",
    "        file.write(\"Score F1 global : {}\\n\".format(f1_score))\n",
    "        file.write(f\"Support : {total_support}\\n\")\n",
    "        file.write(\"\\n\")\n",
    " \n",
    "        # Write results by class\n",
    "        file.write(\"Results per class :\\n\")\n",
    "        for classe in all_classes:\n",
    "            tp = class_TP[classe]\n",
    "            fp = class_FP[classe]\n",
    "            fn = class_FN[classe]\n",
    "            support = tp + fn\n",
    " \n",
    "            recall_class = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "            precision_class = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "            f1_score_class = 2 * (precision_class * recall_class) / (precision_class + recall_class) if (precision_class + recall_class) != 0 else 0\n",
    "            \n",
    "            recall_formated, precision_formated, f1_score_formated = reformated_decimal(tp, fp, fn, recall_class, precision_class, f1_score_class)\n",
    "            table_data.append([classe, tp, fp, fn, precision_formated, recall_formated, f1_score_formated, support])\n",
    "            \n",
    "            \n",
    "            file.write(\"Class {}\\n\".format(classe))\n",
    "            file.write(\"Number of TP: {}\\n\".format(tp))\n",
    "            file.write(\"Number of FP : {}\\n\".format(fp))\n",
    "            file.write(\"Number of FN: {}\\n\".format(fn))\n",
    "            file.write(\"Recall (Recall): {}\\n\".format(recall_class))\n",
    "            file.write(\"Precision : {}\\n\".format(precision_class))\n",
    "            file.write(\"Score F1 : {}\\n\".format(f1_score_class))\n",
    "            file.write(f\"Support : {support}\\n\")\n",
    "            file.write(\"\\n\")\n",
    " \n",
    "    print(f\"The {output_file_txt} file has been created.\")\n",
    "\n",
    "    recall_formated, precision_formated, f1_score_formated = reformated_decimal(total_TP, total_FP, total_FN, recall, precision, f1_score)\n",
    "    table_data.append(['Overall', total_TP, total_FP, total_FN, precision_formated, recall_formated, f1_score_formated, total_support])\n",
    "    \n",
    "    # Generate a PNG file with a table\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    \n",
    "    table = ax.table(cellText=table_data, colLabels=['Classes', 'Nb TP', 'Nb FP', 'Nb FN', 'Precision', 'Rappel', 'Score F1', 'Support'],\n",
    "                     loc='center', cellLoc='center')\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.auto_set_column_width([0, 1, 2, 3, 4, 5, 6,7])\n",
    "    \n",
    "    plt.savefig(output_png, bbox_inches='tight')\n",
    "    plt.show()\n",
    " \n",
    "    print(f\"The {output_png} file has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f322b-691c-49b2-bf57-37699b46c124",
   "metadata": {},
   "source": [
    "## Print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0640e00-89b3-4a4c-b2b2-e7654fc5853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(project_folder:str, yolo_model_folder:str) -> None:\n",
    "    \"\"\"\n",
    "    Generate and save a confusion matrix from YOLO prediction results.\n",
    "\n",
    "    The confusion matrix compares predicted classes to corrected (ground truth) classes\n",
    "    and includes a 'Background' class to handle false positives and false negatives.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    project_folder : str\n",
    "        Path to the project folder.\n",
    "\n",
    "    yolo_model_folder : str\n",
    "        Path to the folder containing the YOLO model and its output data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        A PNG image of the confusion matrix is saved in the results folder.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The CSV file must exist at 'results/results_for_evaluation.csv'.\n",
    "    - NaN values in predictions or corrections are mapped to 'Background'.\n",
    "    - The matrix is saved as 'confusionMatrice.png'.\n",
    "    \"\"\"\n",
    "\n",
    "    results_folder = Path(get_results_folder(project_folder, yolo_model_folder))\n",
    "    csv_with_results = results_folder / 'results'/ 'results_for_evaluation.csv'\n",
    "    if not csv_with_results.exists():\n",
    "        raise FileNotFoundError(f\"No CSV found at {csv_with_results}\")\n",
    "\n",
    "    # Load labels from the labels file\n",
    "    dict_labels = get_labels(str(results_folder / 'labels.txt'))\n",
    "    display_labels=list(dict_labels.values())\n",
    "    \n",
    "    # Add the 'Background' class used for file the NaN results\n",
    "    display_labels.append('Background')\n",
    "\n",
    "    confusion_matrix_path = results_folder / 'results' / 'confusion_matrix.png'\n",
    "    \n",
    "    # Open the csv with results\n",
    "    results = pd.read_csv(csv_with_results, sep=';')\n",
    "\n",
    "\n",
    "    # Replace the NaN results with 'Background', the class will be used to show the FP and FN\n",
    "    predictions = results['Predicted_class'].fillna('Background')\n",
    "    corrections = results['Corrected_class'].fillna('Background')\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    confusion_matix = metrics.confusion_matrix(y_pred=predictions, y_true=corrections, labels=display_labels)\n",
    "    # print(confusion_matix)\n",
    "\n",
    "    # To create a more interpretable visual display we need to convert the table into a confusion matrix display\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matix, display_labels=display_labels)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Costumizing and visualizing the display with rotation of x-axis labels\n",
    "    cm_display.plot(ax=ax, xticks_rotation=90, cmap='autumn', values_format='d')\n",
    "\n",
    "    plt.title('Confusion matrice')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_matrix_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5348ff-9707-4a5e-a48a-0ddaf20cd901",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c78ef067-0373-4914-8f70-75461c1debc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_folder = 'ABSPATHTOTHEFOLDER' # to be changed, asbolute path to a folder with images only, without annotations.\n",
    "yolo_model_folder = 'ABSPATHTOTHEMODELFOLDER' # to be changed, asbolute path to the folder with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d4b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update YOLO label definitions with new classes found in prediction correction files\n",
    "add_new_labels(project_folder, yolo_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43e3e8-8b98-414e-8279-26b81465c88b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the corrected files in YOLO format\n",
    "get_corrected_label_files(project_folder, yolo_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0c483-7322-4d30-9fa1-232dcb27b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a CSV with the corrected data\n",
    "get_csv_results(project_folder, yolo_model_folder, all_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6848544c-9373-4045-9929-e443bcb26ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the file with metrics\n",
    "get_txt_results(project_folder, yolo_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc2c9c-b607-4785-a686-965272901f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "create_confusion_matrix(project_folder, yolo_model_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO_env",
   "language": "python",
   "name": "yolo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
