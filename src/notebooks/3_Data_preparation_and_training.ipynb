{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab90f7f3-427c-452b-bd48-200ddc34b8cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Data preparation and training\n",
    "\n",
    "These scripts are designed to prepare data for YOLO training.\n",
    "\n",
    "The training folder must contain three elements :\n",
    "- a 'labels' folder: in which annotation files are stored,\n",
    "- an 'images' folder: in which image files are stored,\n",
    "- a 'labels.txt' file: containing annotation data in YOLO format: \n",
    "    - '0': 'class 0',/n '1': 'class 1',/n etc.\n",
    "    \n",
    "\n",
    "The *Image transformation* section is designed to double the training dataset by starting from annotated image data, using image warping: images and annotations are deformed according to the same transformation matrix.\n",
    "\n",
    "For each image and annotation file, a new file is generated with the following characteristics:\n",
    "\n",
    "+ A new image with a modified perspective: a new image is created by applying a perspective transformation matrix to the original image.\n",
    "+ A new text file containing annotations adjusted according to the transformation applied to the image: each annotation associated with the original image is also transformed using the same perspective transformation matrix. The new annotation coordinates are then saved in a new text file.\n",
    "\n",
    "The data generated by perspective transformations are labelled \"filename_TP\".\n",
    "\n",
    "All these scripts are designed to process image and text data with the same name (except for the extension) contained in the 'labels' and 'images' folders.\n",
    "\n",
    "You can use unannotated images for the training session. In this case you can create an empty file or no file, the result will be the same: https://github.com/ultralytics/yolov5/discussions/7148#discussioncomment-2440612 \n",
    "\n",
    "**Notice concerning use** \n",
    "Any use, even partial, of the content of this notebook must be accompanied by an appropriate citation.\n",
    "\n",
    "&copy; 2023 Marion Charpier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee6c24-9f4b-41b4-b4a3-82bc16f5004a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db34875-9c39-487a-a0a3-89508534379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join('..', 'modules'))\n",
    "\n",
    "from class_names_functions import get_labels\n",
    "from corners_functions import get_corners, from_corners_to_relative\n",
    "from transform_coordinates_functions import from_relative_coordinates_to_absolute\n",
    "from device_function import which_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11982cc-7b26-4d37-830c-0653252385a5",
   "metadata": {},
   "source": [
    "## Cleaning annotation files (.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "097352de-5bcb-4f4e-a67c-3c9acbce10c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comma(training_folder):\n",
    "    \"\"\"\n",
    "    This function removes any commas that may appear in the annotation `.txt` files within the specified training folder.\n",
    "    This is particularly useful when annotation files are generated or modified from CSV files, \n",
    "    as commas can accidentally be included and cause issues during model training.\n",
    "\n",
    "    :param training_folder: \n",
    "        - Type: str\n",
    "        - Description: The absolute path to the training folder containing the 'labels' subdirectory. \n",
    "                       The function will look for `.txt` files in the 'labels' folder and remove commas from them.\n",
    "\n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It modifies the content of the `.txt` files in place, \n",
    "                       removing any commas that are found.\n",
    "\n",
    "    This function ensures that annotation files are formatted correctly, preventing errors during the training process.\n",
    "    \"\"\"\n",
    "\n",
    "    for filename in os.listdir(os.path.join(training_folder, 'labels')):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(training_folder, 'labels', filename)\n",
    "            # print(file_path)\n",
    "            \n",
    "            # Read the file content\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            # Remove commas\n",
    "            content_without_comma = content.replace(',', '')\n",
    "            \n",
    "            # Write the modified content in the file\n",
    "            with open(file_path, 'w') as file:\n",
    "                file.write(content_without_comma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e054e7d-0e53-42d1-87b4-5025a7b0195d",
   "metadata": {},
   "source": [
    "## Increase dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc58d24-d832-4a5b-bfdf-1354a51f13a1",
   "metadata": {},
   "source": [
    "### Image transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7670edd-ee9c-4493-a5b9-03d31e06fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_transformation(img_file):\n",
    "    \"\"\"\n",
    "    This function applies a perspective transformation to the given image. It generates a new transformed image \n",
    "    with a different perspective by adjusting the image dimensions randomly. The function saves the transformed \n",
    "    image with the same name as the original image, appending '_PT' to indicate the perspective transformation.\n",
    "\n",
    "    :param img_file: \n",
    "        - Type: str\n",
    "        - Description: The absolute path to the image file that needs to be transformed.\n",
    "\n",
    "    :return: \n",
    "        - Type: numpy.ndarray\n",
    "        - Description: Returns the transformation matrix used for the perspective transformation. This matrix \n",
    "                       can be used to understand the geometric changes applied to the image.\n",
    "\n",
    "    This function is useful for data augmentation and generating variations of the original images, which can be \n",
    "    beneficial for training machine learning models with diverse perspectives.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open image and get dimensions\n",
    "    img = cv2.imread(img_file)\n",
    "    rows, cols = img.shape[:2]\n",
    "\n",
    "    # Define the points of origin for the perspective transformation.\n",
    "    # These points form a quadrilateral covering the entire original image.\n",
    "    pts1 = np.float32([[0, 0], [cols, 0], [cols, rows], [0, rows]])\n",
    "\n",
    "    # Generate a new random width and height for the transformed image, between 30% and 80% of the original width.\n",
    "    new_width = random.randint(int(cols*0.3), int(cols*0.8))\n",
    "    new_height = random.randint(int(rows*0.3), int(rows*0.8))\n",
    "\n",
    "    # Define the new points for the perspective transformation\n",
    "    pts2 = np.float32([[0, 0], [new_width, 0], [new_width, new_height], [0, new_height]])\n",
    "\n",
    "    # Calculate the perspective transformation matrix\n",
    "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "\n",
    "    # Apply the perspective transformation to the original image.\n",
    "    dst = cv2.warpPerspective(img, M, (new_width, new_height))\n",
    "\n",
    "    # Save the transformed image in the output folder\n",
    "    transformed_img = Path(img_file).with_suffix('').as_posix() + '_PT' + Path(img_file).suffix\n",
    "    cv2.imwrite(transformed_img, dst)\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eae5cb-d4e4-437c-90ac-d30ce4f24ac8",
   "metadata": {},
   "source": [
    "### Annotations transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1f015-ebb0-40fe-b6d0-a70645e0d751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_transformation_annotation(ann_file, img_file, M):\n",
    "    \"\"\" \n",
    "    This function applies a perspective transformation matrix to the bounding box annotations of an image \n",
    "    and saves the new transformed annotations in a separate file. The transformed annotations correspond to\n",
    "    the modified perspective and dimensions of the image after applying the perspective transformation.\n",
    "\n",
    "    :param ann_file: \n",
    "        - Type: str\n",
    "        - Description: The absolute path to the annotation file (`.txt`) associated with the image. \n",
    "                       The file should contain bounding box annotations in YOLO format \n",
    "                       (label, x_center, y_center, width, height).\n",
    "\n",
    "    :param img_file: \n",
    "        - Type: str\n",
    "        - Description: The absolute path to the original image file. This is used to retrieve \n",
    "                       the original image dimensions and the dimensions of the transformed image.\n",
    "\n",
    "    :param M: \n",
    "        - Type: numpy.ndarray\n",
    "        - Description: The transformation matrix used for perspective transformation. This matrix \n",
    "                       is used to transform the bounding box coordinates to match the new image perspective.\n",
    "\n",
    "    :return: \n",
    "        - Type: list of tuples\n",
    "        - Description: Returns a list of transformed bounding box annotations. Each tuple contains \n",
    "                       the label and new relative coordinates (x_center, y_center, width, height) \n",
    "                       after applying the perspective transformation.\n",
    "\n",
    "    The function ensures that the bounding box annotations remain consistent with the perspective changes applied to the image,\n",
    "    which is essential for maintaining annotation accuracy after transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    img_height, img_width = cv2.imread(img_file).shape[:2]\n",
    "    TP_img_height, TP_img_width = cv2.imread(Path(img_file).with_suffix('').as_posix() + '_PT' + Path(img_file).suffix).shape[:2]\n",
    "    \n",
    "    # print(f\"Origal size: {img_height}, {img_width}\\nNew size: {TP_img_height}, {TP_img_width}\")\n",
    "\n",
    "    # Initialising a list to store the new bounding box coordinates \n",
    "    bb_coordinates = []\n",
    "\n",
    "    with open(ann_file, 'r') as annotations:\n",
    "        for annotation in annotations:\n",
    "\n",
    "            # Extraire les coordonnées de l'annotation\n",
    "            label, x_center, y_center, width, height = annotation.split()\n",
    "            \n",
    "            #print(type(label), type(x_center), type(y_center), type(width), type(height))\n",
    "\n",
    "            # Convertir les coordonnées relatives en coordonnées absolues\n",
    "            corners = get_corners(x_center, y_center, width, height, img_width, img_height)\n",
    "\n",
    "            # Appliquer la transformation aux coins de la boîte d'annotation\n",
    "            corners = np.array(corners, dtype=np.float32).reshape(-1, 1, 2)\n",
    "            transformed_corners = cv2.perspectiveTransform(corners, M).reshape(-1, 2)\n",
    "\n",
    "            # Calculer les nouvelles coordonnées relatives\n",
    "            new_upper_left = transformed_corners[0]\n",
    "            new_bottom_right = transformed_corners[2]\n",
    "\n",
    "            #print(f'new_upper_left = {new_upper_left}, new_bottom_right = {new_bottom_right}')\n",
    "\n",
    "            # Transformer les nouvelles coordonnés en relatives\n",
    "            transformed_x_center, transformed_y_center, transformed_width, transformed_height = from_corners_to_relative(\n",
    "                new_upper_left, new_bottom_right, TP_img_width, TP_img_height)\n",
    "\n",
    "            bb_coordinates.append((label, transformed_x_center, transformed_y_center, transformed_width, transformed_height))\n",
    "\n",
    "    new_annotation = Path(ann_file).with_suffix('').as_posix() + '_PT' + Path(ann_file).suffix\n",
    "\n",
    "    with open(new_annotation, 'w') as transformed_annotations:\n",
    "        for bb in bb_coordinates:\n",
    "            label, x, y, w, h = bb\n",
    "            transformed_annotations.write(f\"{label} {x} {y} {w} {h}\\n\")\n",
    "\n",
    "\n",
    "    return bb_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b31504-30e0-408a-b03b-986151135bd6",
   "metadata": {},
   "source": [
    "### Generate transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a765f94d-db8d-4a8a-86b4-a5671ea90072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transformed_data(training_folder):\n",
    "    \"\"\"\n",
    "    This function generates a set of transformed images and their corresponding annotations by applying \n",
    "    perspective transformations to each image and adjusting the bounding box annotations accordingly. \n",
    "    The new images and annotations are saved in the appropriate folders within the specified training folder.\n",
    "\n",
    "    :param training_folder: \n",
    "        - Type: str\n",
    "        - Description: The absolute path to the training folder containing the dataset. The folder should \n",
    "                       have subdirectories named 'images' for storing image files and 'labels' for storing \n",
    "                       the corresponding annotation files.\n",
    "\n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It generates transformed images and annotation files \n",
    "                       in place, saving them in the same subdirectories with modified filenames.\n",
    "\n",
    "    This function automates the data augmentation process by generating new variations of the dataset, \n",
    "    which can be used to enhance model robustness during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Images tranformation has started..')\n",
    "    \n",
    "    img_folder = os.path.join(training_folder, 'images')\n",
    "    annotations_folder = os.path.join(training_folder, 'labels')\n",
    "\n",
    "    images = [img for img in os.listdir(img_folder) if img.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    for img in images:\n",
    "        img_file = os.path.join(img_folder, img)\n",
    "        ann_file = os.path.join(annotations_folder, Path(img).stem + '.txt')\n",
    "        \n",
    "        if os.path.exists(ann_file):\n",
    "            M = perspective_transformation(img_file)\n",
    "            perspective_transformation_annotation(ann_file, img_file, M)\n",
    "\n",
    "    print(f'New images stored in {img_folder}\\nNew annotations stored in {annotations_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8d8581-a974-496c-9c1b-6eee1f15dd38",
   "metadata": {},
   "source": [
    "## Create the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c1e51a4-c1dc-4f16-9eb7-d5744872c7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_training_dataset(training_folder, pretrained_model, Preexisting_Distribution):\n",
    "    \"\"\"\n",
    "    This function prepares the training and validation datasets by creating text files that list the images used for each subset.\n",
    "    It generates three text files in the 'dataset_statistics' subdirectory of the training folder:\n",
    "\n",
    "    1. `traindata.txt`: Contains the list of images used for training (80% of the data).\n",
    "    2. `valdata.txt`: Contains the list of images used for validation (20% of the data).\n",
    "    3. `training_dataset.txt`: Contains the list of all images used (both training and validation).\n",
    "\n",
    "    If a pre-existing distribution of training and validation data is provided, the function uses it instead \n",
    "    of creating new text files. The function then splits the data into subdirectories for training and \n",
    "    validation based on the text files.\n",
    "\n",
    "    :param training_folder: \n",
    "        - Type: str\n",
    "        - Description: The absolute path to the folder containing the dataset. The folder should include \n",
    "                       an 'images' subdirectory for image files and a 'labels' subdirectory for corresponding \n",
    "                       annotation files.\n",
    "\n",
    "    :param pretrained_model: \n",
    "        - Type: str\n",
    "        - Description: The absolute path to the folder containing the pre-trained model data. \n",
    "                       This parameter is currently unused in the function but may be required for future enhancements.\n",
    "\n",
    "    :param Preexisting_Distribution: \n",
    "        - Type: bool\n",
    "        - Description: If `True`, the function will create new text files for training, validation, \n",
    "                       and the entire dataset. If a path is provided, the function will use the pre-existing \n",
    "                       distribution from that path instead of creating new files.\n",
    "\n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It creates and saves the text files \n",
    "                       `traindata.txt`, `valdata.txt`, and `training_dataset.txt` and organizes the images \n",
    "                       and labels into separate subdirectories for training and validation.\n",
    "\n",
    "    This function ensures that the training and validation data are correctly organized and ready for model training.\n",
    "    \"\"\"\n",
    "\n",
    "    folder_base = os.path.dirname(training_folder)\n",
    "    dataset_name = os.path.basename(training_folder)\n",
    "    \n",
    "    # Folder in which all training-related files are stored\n",
    "    stat_folder = os.path.join(training_folder, 'dataset_statistics')\n",
    "    \n",
    "    if Preexisting_Distribution:\n",
    "        print(f'Use pre-existing files from {Preexisting_Distribution}.')\n",
    "        shutil.copytree(os.path.join(Preexisting_Distribution, 'dataset_statistics'), stat_folder)\n",
    "   \n",
    "    else:\n",
    "        # Get a list of the images\n",
    "        files = os.listdir(os.path.join(training_folder, 'images'))\n",
    "\n",
    "        # Filter file names to keep only those with \".jpg\" and \".png\" extensions\n",
    "        image_files = [f for f in files if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "\n",
    "        # Shuffle file names randomly\n",
    "        random.shuffle(image_files)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Ajouter ici le sklearn pour split avec équilibre des classes\n",
    "        \"\"\"\n",
    "        # Calcul le nombre d'images pour chaque ensemble\n",
    "        num_images = len(image_files)\n",
    "        num_train = int(num_images * 0.8)\n",
    "        num_val = int(num_images - num_train)\n",
    "\n",
    "        # Divide file names into two sets : one for the training, one for the validation\n",
    "        train_files = image_files[:num_train]\n",
    "        val_files = image_files[num_train:num_train+num_val]\n",
    "        \n",
    "        # Check if the destination folder exists, if not create it\n",
    "        os.makedirs(stat_folder, exist_ok=True)\n",
    "\n",
    "        # Create a file with the list for the train data\n",
    "        with open(os.path.join(stat_folder, 'traindata.txt'), 'w') as f:\n",
    "            for image_file in train_files:\n",
    "                f.write(os.path.join(training_folder, 'image_inputs/ground_truth_images', image_file) + \"\\n\")\n",
    "        print(f\"File create in {os.path.join(stat_folder, 'traindata.txt')}\")\n",
    "\n",
    "        # Create a file with the list for valdidation data\n",
    "        with open(os.path.join(stat_folder,'valdata.txt'), 'w') as f:\n",
    "            for image_file in val_files:\n",
    "                f.write(os.path.join(training_folder, 'image_inputs/ground_truth_images', image_file) + \"\\n\")\n",
    "        print(f\"File create in {os.path.join(stat_folder, 'valdata.txt')}\")\n",
    "\n",
    "\n",
    "        # Create a file with all the dataset\n",
    "        with open(os.path.join(stat_folder, 'training_dataset.txt'), 'w') as f:\n",
    "            for image_file in image_files:\n",
    "                    f.write(os.path.join(training_folder, 'image_inputs/ground_truth_images', image_file) + \"\\n\")\n",
    "            print(f\"File create {os.path.join(stat_folder, 'training_dataset.txt')}\")\n",
    "    \n",
    "\n",
    "    # Split images and txt files into folders from a .txt file\n",
    "    split_data_for_training(os.path.join(stat_folder, 'traindata.txt'), \n",
    "                            os.path.join(training_folder, 'labels'), \n",
    "                            os.path.join(folder_base, 'datasets', dataset_name, 'images/train'), \n",
    "                            os.path.join(folder_base, 'datasets', dataset_name, 'labels/train'))\n",
    "    \n",
    "    split_data_for_training(os.path.join(stat_folder,'valdata.txt'),\n",
    "                            os.path.join(training_folder, 'labels'),\n",
    "                            os.path.join(folder_base, 'datasets', dataset_name, 'images/val'),\n",
    "                            os.path.join(folder_base, 'datasets', dataset_name, 'labels/val'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f650486-2ca6-4a0e-94b7-a9a31b0508d1",
   "metadata": {},
   "source": [
    "## Split the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b0b1d63-f392-4cca-ac3d-bb54184a9c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data_for_training(txt_list, txt_folder, output_img_folder, output_txt_folder):\n",
    "    \"\"\"\n",
    "    This function organizes images and annotation files into the appropriate subdirectories required by YOLOv8 \n",
    "    for training and validation. It moves the images and annotations listed in the specified `.txt` file \n",
    "    into designated 'train' or 'val' folders for both images and labels. This function simplifies the preparation \n",
    "    of datasets by automating the process of organizing images and annotations into the required structure for model training.\n",
    "\n",
    "    :param txt_list: \n",
    "        - Type: str\n",
    "        - Description: The path to the `.txt` file containing the list of image file paths to be used. Each line \n",
    "                       in the file should contain the absolute path to an image file.\n",
    "\n",
    "    :param txt_folder: \n",
    "        - Type: str\n",
    "        - Description: The path to the folder where the corresponding `.txt` annotation files are stored. \n",
    "                       These annotation files should have the same base name as the images but with `.txt` extensions.\n",
    "\n",
    "    :param output_img_folder: \n",
    "        - Type: str\n",
    "        - Description: The path to the folder where the image files should be moved. This folder should be organized \n",
    "                       into 'train' or 'val' subdirectories under an 'images' folder as required by YOLO.\n",
    "\n",
    "    :param output_txt_folder: \n",
    "        - Type: str\n",
    "        - Description: The path to the folder where the annotation files should be moved. This folder should be \n",
    "                       organized into 'train' or 'val' subdirectories under a 'labels' folder as required by YOLO.\n",
    "\n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It moves the images and annotations into the specified \n",
    "                       directories and organizes them for training and validation.\n",
    "\n",
    "    The function will move the images and annotations to the respective train or val folders under the `output_img_folder` \n",
    "    and `output_txt_folder` paths, organizing them for YOLO training.    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Create the output folder if it does not already exist\n",
    "    os.makedirs(output_img_folder, exist_ok=True)\n",
    "    os.makedirs(output_txt_folder, exist_ok=True)\n",
    "    \n",
    "    folder_base = os.path.dirname(training_folder)\n",
    "    dataset_name = os.path.basename(training_folder)\n",
    "    \n",
    "    # Open the text file containing the image paths\n",
    "    with open(txt_list, \"r\") as f:\n",
    "        # Browse through each line of the file\n",
    "        for line in f:\n",
    "            # Get the image path and text file name\n",
    "            image_path = line.strip()\n",
    "            image_name = os.path.basename(image_path)\n",
    "\n",
    "            txt_file = os.path.join(txt_folder, image_name).replace(image_name.split('.')[-1], 'txt')\n",
    "            \n",
    "            # Copy image to output folder\n",
    "            shutil.move(image_path, os.path.join(output_img_folder, os.path.basename(image_path)))\n",
    "        \n",
    "            # Copy text file to output folder\n",
    "            try:\n",
    "                shutil.move(txt_file, os.path.join(output_txt_folder, os.path.basename(txt_file)))\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(f'Text file {txt_file} does not exist')\n",
    "    print(f'Image files move in {output_img_folder}')\n",
    "    print(f'Text files move in {output_txt_folder}')\n",
    "    \n",
    "    # Create the yaml file\n",
    "    write_yaml_file(training_folder, dataset_name, folder_base)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7257e4d-d6ad-443d-9ff9-9587eb34c299",
   "metadata": {},
   "source": [
    "## Create the .yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f09c1055-68d4-4cdb-bd74-4c6006d07056",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_yaml_file(training_folder, dataset_name, folder_base):\n",
    "    \"\"\"\n",
    "    This function creates a `.yaml` configuration file that specifies the paths to the training and validation \n",
    "    datasets, as well as the class labels used for model training. This file is essential for training YOLOv8 models, \n",
    "    as it provides information on the dataset structure and class mappings.\n",
    "    The file will contain paths to the training and validation data, along with class names.\n",
    "\n",
    "    :param training_folder: \n",
    "        - Type: str\n",
    "        - Description: The absolute path to the training folder containing the `labels.txt` file. \n",
    "                       This file is used to retrieve the class labels for the dataset.\n",
    "\n",
    "    :param dataset_name: \n",
    "        - Type: str\n",
    "        - Description: The name of the training session or dataset. This name is used as the base name \n",
    "                       for the `.yaml` file and to construct the paths to the dataset.\n",
    "\n",
    "    :param folder_base: \n",
    "        - Type: str\n",
    "        - Description: The absolute path to the root directory where the `datasets` folder is located. \n",
    "                       This folder will contain the training data organized in the required structure.\n",
    "\n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It creates a `.yaml` file in the dataset directory \n",
    "                       with the specified configuration.\n",
    "                    \n",
    "    This `.yaml` file can then be used to train a YOLO model by specifying it as the configuration file during training. \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Get the annotations classes\n",
    "    annotation_classes = get_labels(os.path.join(training_folder, 'labels.txt'))\n",
    "    \n",
    "    # Convertir les clés du dictionnaire annotation_classes en entiers\n",
    "    annotation_classes_int = {int(key): value for key, value in annotation_classes.items()}\n",
    "\n",
    "    # Formater la chaîne avec les éléments dans l'ordre souhaité\n",
    "    yaml_data = f\"path: {os.path.join(folder_base, 'datasets', dataset_name)}/\\n\" \\\n",
    "                f\"train: 'images/train'\\n\" \\\n",
    "                f\"val: 'images/val'\\n\" \\\n",
    "                f\"\\n\" \\\n",
    "                f\"#class names\\n\" \\\n",
    "                f\"names:\\n\"\n",
    "    for key, value in annotation_classes_int.items():\n",
    "        yaml_data += f\"  {key}: '{value}'\\n\"\n",
    "        \n",
    "        # Ajouter Albumentation\n",
    "\n",
    "    with open(os.path.join(folder_base, 'datasets', dataset_name, dataset_name + '.yaml'), 'w') as yaml_file:\n",
    "        yaml_file.write(yaml_data)\n",
    "    print(f\"File edit in {os.path.join(folder_base, 'datasets', dataset_name, dataset_name + '.yaml')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ba4cd-693d-4a1a-8198-3d213c12916d",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d299c9f-8939-4dac-90a4-6142fac6b396",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def yolo_training(training_folder, use_model, img_size, epochs, batch, workers, label_smoothing, pretrained_model):\n",
    "    \"\"\"\n",
    "    This function initiates the training process for a YOLO model using the specified training parameters and dataset.\n",
    "    The function provides flexibility in selecting the model architecture, adjusting training settings, \n",
    "    and optionally using a pre-trained model to fine-tune the results.\n",
    "\n",
    "    :param training_folder: \n",
    "        - Type: str\n",
    "        - Description: The path to the folder containing the training data. This folder should include \n",
    "                       a `.yaml` file specifying the dataset paths and class names.\n",
    "\n",
    "    :param use_model: \n",
    "        - Type: str\n",
    "        - Description: The YOLO model architecture to use for training (e.g., 'yolo11x.pt'). \n",
    "                       If a pre-trained model is provided, this parameter is overridden.\n",
    "\n",
    "    :param img_size: \n",
    "        - Type: int\n",
    "        - Description: The size of the input images. Larger image sizes can increase model accuracy \n",
    "                       but may also increase computational load.\n",
    "\n",
    "    :param epochs: \n",
    "        - Type: int\n",
    "        - Description: The number of epochs to train the model. More epochs allow the model to learn \n",
    "                       better but may result in overfitting if set too high.\n",
    "\n",
    "    :param batch: \n",
    "        - Type: int\n",
    "        - Description: The batch size to use during training. Larger batch sizes require more memory \n",
    "                       but can stabilize gradient updates.\n",
    "\n",
    "    :param workers: \n",
    "        - Type: int\n",
    "        - Description: The number of workers for data loading. Increasing this number can speed up data \n",
    "                       loading but may require more computational resources.\n",
    "\n",
    "    :param label_smoothing: \n",
    "        - Type: float\n",
    "        - Description: The smoothing factor applied to the labels to prevent overconfidence in predictions. \n",
    "                       Typically set between 0 and 1.\n",
    "\n",
    "    :param pretrained_model: \n",
    "        - Type: str\n",
    "        - Description: The path to a pre-trained model, if any. If provided, the function will use this model \n",
    "                       as the starting point for training. If not provided, it uses the `use_model` parameter \n",
    "                       to select the model architecture.\n",
    "\n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It trains the YOLO model using the provided \n",
    "                       parameters and saves the results to a specified output folder.\n",
    "\n",
    "    This function automates the YOLO training process, providing flexibility in configuration and managing results storage.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Derive additional paths and model name\n",
    "    folder_base = os.path.dirname(training_folder)\n",
    "    dataset_name = os.path.basename(training_folder)\n",
    "    common_base = os.path.commonpath([training_folder, os.getcwd()])\n",
    "    \n",
    "    date = datetime.now().strftime('%Y%m%d')\n",
    "    yaml_file = os.path.join(folder_base, 'datasets', dataset_name, dataset_name + '.yaml')\n",
    "\n",
    "    # Check if yaml_file exists\n",
    "    if not os.path.exists(yaml_file):\n",
    "        raise FileNotFoundError(f\"YAML file not found: {yaml_file}\")\n",
    "\n",
    "    # Determine which model to use\n",
    "    if pretrained_model == '':\n",
    "        use_model = use_model\n",
    "        model_name = f'{dataset_name}_{date}_{use_model[-5:-3]}_i{img_size}_e{epochs}_b{batch}_w{workers}'\n",
    "    else:\n",
    "        use_model = pretrained_model\n",
    "        model_name = f'{dataset_name}_{date}_{use_model[-7:-3]}_i{img_size}_e{epochs}_b{batch}_w{workers}'\n",
    "\n",
    "    # Check if the GPU is available - if not, use the CPU\n",
    "    device = which_device()\n",
    "    \n",
    "    # Load a YOLO model\n",
    "    model = YOLO(use_model, task='detect').to(device)\n",
    "\n",
    "    # Train the model\n",
    "    results = model.train(\n",
    "       data = yaml_file, # path to the datasets and classes\n",
    "       imgsz = img_size, #image size\n",
    "       epochs = epochs,\n",
    "       batch = batch,\n",
    "       label_smoothing = label_smoothing,\n",
    "       workers = workers, # increases training speed, default setting is 8\n",
    "       name = model_name, # output folder\n",
    "       project = os.path.join(common_base, 'output', 'runs', 'train')\n",
    "    )\n",
    "\n",
    "    # Evaluate the model's performance on the validation set\n",
    "    results = model.val(\n",
    "        name = model_name + '/'+ model_name +'_val')\n",
    "    \n",
    "    print(f\"Training completed. Results saved to {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f36fa-fd39-4dc6-9229-aa6c4037aaae",
   "metadata": {},
   "source": [
    "### Resuming interrupted trainings(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0ad3b74-b5f5-4e59-825b-aba110229f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resume_training(interrupted_model_folder):\n",
    "    \"\"\"\n",
    "    This function resumes an interrupted YOLO model training session from the last saved checkpoint. \n",
    "    It retrieves the last trained weights and other session parameters to continue training without \n",
    "    losing previously achieved progress.\n",
    "    This function is useful for resuming training sessions that were stopped due to hardware limitations, \n",
    "    time constraints, or other interruptions.\n",
    "\n",
    "    :param interrupted_model_folder: \n",
    "        - Type: str\n",
    "        - Description: The path to the folder containing the partially trained model's data. \n",
    "                       This folder should include a `weights` subdirectory with the `last.pt` file \n",
    "                       (the last saved weights).\n",
    "\n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It resumes the training process from \n",
    "                       the last checkpoint and evaluates the model's performance after training.\n",
    "\n",
    "    The results of the resumed training and validation will be stored in the same directory, maintaining \n",
    "    the continuity of the training session and its associated metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    last_weight = os.path.join(interrupted_model_folder, 'weights/last.pt')\n",
    "    model_name = os.path.basename(interrupted_model_folder)\n",
    "\n",
    "    # Check if the GPU is available - if not, use the CPU\n",
    "    device = which_device()\n",
    "    \n",
    "    # Load a model\n",
    "    model = YOLO(last_weight).to(device)  # load a partially trained model\n",
    "\n",
    "    # Resume training\n",
    "    results = model.train(resume=True)\n",
    "\n",
    "    # Evaluate the model's performance on the validation set\n",
    "    results = model.val(\n",
    "        name = model_name + '/'+ model_name +'_val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ab296-0f02-4332-87d6-8c1d7c8ed3b1",
   "metadata": {},
   "source": [
    "## Re-arrange in pristine state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8afb066e-962c-41bf-a3ba-bc8e4bb4b802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dispatch_data(training_folder, pretrained_model, interrupted_model_folder):\n",
    "    \"\"\"\n",
    "    This function organizes and finalizes the data used for training by moving relevant files and directories \n",
    "    into the model folder. It also restores the original structure of the dataset folder by moving image and \n",
    "    annotation files back to their respective subdirectories and deletes the temporary training folder.\n",
    "\n",
    "    :param training_folder: \n",
    "        - Type: str\n",
    "        - Description: The absolute path to the folder where the dataset used for training is stored. \n",
    "                       This folder should contain subdirectories like 'images' and 'labels' used during training.\n",
    "\n",
    "    :param pretrained_model: \n",
    "        - Type: str\n",
    "        - Description: The path to a pre-trained model, if any. This is used to determine the model folder \n",
    "                       structure for storing training data and configurations.\n",
    "\n",
    "    :param interrupted_model_folder: \n",
    "        - Type: str\n",
    "        - Description: The path to the folder containing data from a previously interrupted training session. \n",
    "                       If provided, the function will continue organizing data into this folder.\n",
    "\n",
    "    :return: \n",
    "        - Type: None\n",
    "        - Description: This function does not return a value. It organizes and moves files into appropriate folders, \n",
    "                       restores the original dataset structure, and deletes the temporary training folder.\n",
    "\n",
    "    This function ensures that all data and configurations used for training are stored in a dedicated model folder, \n",
    "    making it easy to track and manage different training sessions.\n",
    "    \"\"\"\n",
    "\n",
    "    folder_base = os.path.dirname(training_folder)\n",
    "    dataset_name = os.path.basename(training_folder)\n",
    "    common_base = os.path.commonpath([training_folder, os.getcwd()])\n",
    "    \n",
    "    date = datetime.now().strftime('%Y%m%d')\n",
    "    \n",
    "    # Determine which model name to use\n",
    "    if interrupted_model_folder:\n",
    "        model_folder = interrupted_model_folder\n",
    "    else:\n",
    "        # Déterminer quel nom de modèle utiliser\n",
    "        if not pretrained_model:\n",
    "            model_name = f'{dataset_name}_{date}_{use_model[-5:-3]}_i{img_size}_e{epochs}_b{batch}_w{workers}'\n",
    "        else:\n",
    "            model_name = f'{dataset_name}_{date}_{pretrained_model[-7:-3]}_i{img_size}_e{epochs}_b{batch}_w{workers}'\n",
    "        \n",
    "        model_folder = os.path.join(common_base, 'output', 'runs/train', model_name)\n",
    "        \n",
    "    # Move the data used for the training session into the model folder\n",
    "    shutil.move(os.path.join(folder_base, 'datasets', dataset_name, dataset_name + '.yaml'), os.path.join(model_folder, dataset_name + '.yaml'))\n",
    "    print(f'The .yaml file has been moved into {model_folder}')\n",
    "    \n",
    "    shutil.copyfile(os.path.join(training_folder, 'labels.txt'), os.path.join(model_folder, 'labels.txt'))\n",
    "    print(f'The labels.txt file has been copied in {model_folder}')\n",
    "    \n",
    "    shutil.move(os.path.join(training_folder, 'dataset_statistics'), model_folder)\n",
    "    print(f'The statistics folder with the training data have been moved to {model_folder}.')\n",
    "  \n",
    "    # Replace the data in the dataset folder\n",
    "    img_folder_train = os.path.join(folder_base, 'datasets', dataset_name, 'images/train') \n",
    "    txt_folder_train = os.path.join(folder_base, 'datasets', dataset_name, 'labels/train')\n",
    "    img_folder_val = os.path.join(folder_base, 'datasets', dataset_name, 'images/val')\n",
    "    txt_folder_val = os.path.join(folder_base, 'datasets', dataset_name, 'labels/val')\n",
    "\n",
    "    os.makedirs(os.path.join(training_folder, 'images'), exist_ok=True)\n",
    "    for file in os.listdir(img_folder_train):\n",
    "        shutil.move(os.path.join(img_folder_train, file), os.path.join(training_folder, 'images', file))\n",
    "    print(f\"Files from {img_folder_train} have been moved into {os.path.join(training_folder, 'images')}\")\n",
    "    \n",
    "    for file in os.listdir(img_folder_val):\n",
    "        shutil.move(os.path.join(img_folder_val, file), os.path.join(training_folder, 'images', file))\n",
    "    print(f\"Files from {img_folder_val} move into {os.path.join(training_folder, 'images')}\")\n",
    "\n",
    "    os.makedirs(os.path.join(training_folder, 'labels'), exist_ok=True)\n",
    "    for file in os.listdir(txt_folder_train):\n",
    "        shutil.move(os.path.join(txt_folder_train, file), os.path.join(training_folder, 'labels', file))\n",
    "    print(f\"Files from {txt_folder_train} move into {os.path.join(training_folder, 'labels')}\")\n",
    "    \n",
    "    for file in os.listdir(txt_folder_val):\n",
    "        shutil.move(os.path.join(txt_folder_val, file), os.path.join(training_folder, 'labels', file))\n",
    "    print(f\"Files from {txt_folder_val} move into {os.path.join(training_folder, 'labels')}\")\n",
    "\n",
    "    shutil.rmtree(os.path.join(folder_base, 'datasets', dataset_name))\n",
    "    print(f\"The {os.path.join(folder_base, 'datasets', dataset_name)} has been deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46759553-9332-4938-af01-3a8a0c179a33",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5197c-5312-4a46-8974-f24cf2a3efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folder = 'TRAINING_FOLDER' # to be changed, absolute path to the dataset you will use for the training session\n",
    "\n",
    "# Set to the absolute path of the pretrained model if you want to use pretrained model\n",
    "pretrained_model = ''\n",
    "# To change if you want to use a pre-existing distribution for a training session\n",
    "Preexisting_Distribution = '' # absolute path to the model folder containing the distribution to be reused\n",
    "\n",
    "# to change if you want to use pre-existing files or if you want to resume an uncompleted training session\n",
    "interrupted_model_folder = '' # to be changed, absolute path to the model folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa1ca5-16a8-4341-b643-89e6535f3d7c",
   "metadata": {},
   "source": [
    "### Clean, increase and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7ad5c3b-9e17-42d4-8313-b7e69e2c1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the file txt if needed\n",
    "clean_comma(training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13381455-a63d-48db-bb52-8ca57c9296f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%prun\n",
    "# Use the perspective transformation to extend the dataset\n",
    "generate_transformed_data(training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e7bf7-9f5f-4745-a764-4d7999c26969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data distribution file for train/val sets\n",
    "create_training_dataset(training_folder, pretrained_model, Preexisting_Distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faef8d5-8d19-4d9a-9663-b3006efd81c8",
   "metadata": {},
   "source": [
    "### Start a training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f26d4889-bda8-4246-9ffe-433fca7f097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_model = 'yolo11n.pt' # to be changed as needed, by default use 'yolov11x.pt'\n",
    "img_size = 640 # to be changed as needed, by default use 640\n",
    "epochs = 10 # to be changed as needed\n",
    "batch = -1 # to be changed as needed, by default use 8 or or -1 for AutoBatch\n",
    "workers = 8 # to be changed as needed, by default 24, or 8 (https://docs.ultralytics.com/modes/train/#train-settings)\n",
    "label_smoothing = 0.1 # to be changed as needed,by default 0. Can improve generalization\n",
    "dropout = 0.1 # Elimine aléatoirement 10% connaissance à chaque époque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd918a0-d728-4bf6-bb3f-4d3d9cbede6f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start a training session\n",
    "yolo_training(training_folder, use_model, img_size, epochs, batch, workers, label_smoothing, pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99f24af-f762-4ec0-a19d-36f4f8abbe54",
   "metadata": {},
   "source": [
    "### Resume an uncompleted training session (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8709f993-f1ba-4f00-a454-53ca4ebb6bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume an interrupted training\n",
    "#resume_training(interrupted_model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74460189-6de3-452a-8713-ae5952dabf0a",
   "metadata": {},
   "source": [
    "### Dispatch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae5b7b-d9ce-408c-b677-c8d2496ff2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the .txt files describing the distribution of images/labels in train and val of the training data into the model folder and replace the image/label data themself in their original folders\n",
    "dispatch_data(training_folder, pretrained_model, interrupted_model_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiamat_env",
   "language": "python",
   "name": "tiamat_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
